{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Neuron:\n",
    "    def __init__(self,w1,w2,b,name):\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        self.b = b\n",
    "        self.dw1 = 0\n",
    "        self.dw2 = 0\n",
    "        self.db = 0\n",
    "        self.dx1 = 0\n",
    "        self.dx2 = 0\n",
    "        self.name = name\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        z = self.w1 * x1 + self.w2 * x2 + self.b\n",
    "     \n",
    "        a = 1/(1+math.exp(-z))\n",
    "        #print(self.name,' a=', a)\n",
    "        #print(self.name,' dw1=', self.dw1)\n",
    "        #print(self.name,' dw2=', self.dw2)\n",
    "        self.db = a * (1-a)       # Diese Werte merken wir uns f√ºr die backword-progagation\n",
    "        self.dw1 = x1 * self.db\n",
    "        self.dw2 = x2 * self.db\n",
    "        self.dx1 = self.w1 * self.db\n",
    "        #print(self.name, x1, x2)\n",
    "        self.dx2 = self.w2 * self.db\n",
    "        return a\n",
    "    \n",
    "    def status(self):\n",
    "        return 'Neuron {}\\nw1={:4.3f}, w2={:4.3f}, b={:4.3f} \\n'.format(self.name, self.w1, self.w2, self.b) + \\\n",
    "               'dw1={:4.3f}, dw2={:4.3f}, db={:4.3f}'.format(self.dw1, self.dw2, self.db)\n",
    "\n",
    "\n",
    "class Netz:\n",
    "    def __init__(self, param):\n",
    "        w11, w12, b1, w21, w22, b2, w31, w32, b3 = param\n",
    "        self.n1 = Neuron(w11,w12,b1,'n1')\n",
    "        self.n2 = Neuron(w21,w22,b2,'n2')\n",
    "        self.n3 = Neuron(w31,w32,b3,'n3')\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        a1 = self.n1.forward(x1,x2)\n",
    "        a2 = self.n2.forward(x1,x2)\n",
    "        return self.n3.forward(a1,a2)\n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n1 0 0\n",
      "n2 0 0\n",
      "n3 0.8807970779778823 0.2689414213699951\n",
      "n1 0 1\n",
      "n2 0 1\n",
      "n3 0.7310585786300049 0.5\n",
      "n1 1 0\n",
      "n2 1 0\n",
      "n3 0.9525741268224334 0.7310585786300049\n",
      "n1 1 1\n",
      "n2 1 1\n",
      "n3 0.8807970779778823 0.8807970779778823\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X  = [(0,0),(0,1),(1,0),(1,1)]\n",
    "Y  = [0,1,1,0]\n",
    "lr = 0.1\n",
    "param = [1, -1, 2, 2, 1, -1, 1, 2, 2]\n",
    "w11,w12,b1,w21,w22,b2,w31,w32,b3 = param\n",
    "\n",
    "for i in range(1):\n",
    "    netz = Netz(param)\n",
    "    dw11, dw12, db1, dw21, dw22, db2, dw31, dw32, db3 = [0] * 9\n",
    "    for i in range(4):\n",
    "        x1, x2, y = X[i][0], X[i][1], Y[i]\n",
    "        a = netz.forward(x1, x2)\n",
    "      \n",
    "        n1, n2, n3 = netz.n1, netz.n2, netz.n3\n",
    "        ay = (a - Y[i])\n",
    "        #print(ay)\n",
    "        db3  += ay * n3.db\n",
    "        #print(ay,n3.dw1)\n",
    "        dw31 += ay * n3.dw1\n",
    "        dw32 += ay * n3.dw2\n",
    "        \n",
    "        db2  += ay * n3.dx2 * n2.db\n",
    "        dw21 += ay * n3.dx2 * n2.dw1\n",
    "        dw22 += ay * n3.dx2 * n2.dw2\n",
    "        \n",
    "        db1  += ay * n3.dx2 * n1.db\n",
    "        dw11 += ay * n3.dx2 * n1.dw1\n",
    "        dw12 += ay * n3.dx2 * n1.dw2\n",
    "        \n",
    "    \n",
    "    #print(dw31, dw32, db3)\n",
    "    w11 -= (lr * dw11)\n",
    "    w12 -= (lr * dw12)\n",
    "    b1 -= (lr * db1)\n",
    "    w21 -= (lr * dw21)\n",
    "    w22 -= (lr * dw22)\n",
    "    b2 -= (lr * db2)\n",
    "    w31 -= (lr * dw31)\n",
    "    w32 -= (lr * dw32)\n",
    "    b3 -= (lr * db3)\n",
    "    param = [w11,w12,b1,w21,w22,b2,w31,w32,b3]\n",
    "   \n",
    "    \n",
    "#for x1, x2 in X:\n",
    "#    print(x1, x2, netz.forward(x1, x2))\n",
    "\n",
    "#print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.69675472 1.11236132]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3) # set a seed so that the results are consistent\n",
    "X  = np.array([(0,0),(0,1),(1,0),(1,1)]) \n",
    "Y  = np.array([0,1,1,0]).reshape(4,1,1)\n",
    "\n",
    "X = X.T.reshape(2,4)\n",
    "Y = Y.reshape(1,4)\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_iterations = 1\n",
    "\n",
    "n_x = 2   # size of features (input layer)\n",
    "n_h = 2   # size of hidden layer\n",
    "n_y = 1   # size auf output layer\n",
    "m = Y.shape[1] # number of example\n",
    "\n",
    "# initialize Parameter\n",
    "#W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "W1 = np.array([[1,-1],[2,1]])\n",
    "b1 = np.array([[2],[-1]]) \n",
    "#b1 = np.zeros((n_h,1))\n",
    "#W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "W2 = np.array([[1,2]])\n",
    "#b2 =  np.zeros((n_y,1))\n",
    "b2 = np.array([[2]])\n",
    "\n",
    "'''\n",
    "print(\"hidden-Layer\")\n",
    "print(W1,b1)\n",
    "print(\"output-Layer\")\n",
    "print(W2,b2)\n",
    "'''    \n",
    "for i in range(num_iterations):\n",
    "    # forward propagation\n",
    "    Z1 = np.matmul(W1,X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.matmul(W2,A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    " \n",
    "   \n",
    "    # calculate cost\n",
    "    #logprobs = np.multiply(np.log(A2),Y) + np.multiply(1 - Y,np.log(1-A2))\n",
    "    #cost = - 1/m * (np.sum(logprobs))\n",
    "    #cost = np.squeeze(cost)  \n",
    "\n",
    "    # backward propagation\n",
    "    dZ2 = A2 - Y\n",
    "    \n",
    "    dW2 = np.matmul(dZ2,A1.T)\n",
    "    print(dW2)\n",
    " \n",
    "    db2 = np.sum(dZ2, axis = 1, keepdims = True)\n",
    "    \n",
    "    dZ1 = np.multiply(np.matmul(W2.T,dZ2),(1 - np.power(A1, 2)))\n",
    "    dW1 = np.matmul(dZ1,X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis = 1, keepdims = True)\n",
    "\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    #print(\"hidden-Layer\")\n",
    "    #print(W1,b1)\n",
    "    #print(\"output-Layer\")\n",
    "    #print(W2,b2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param = [1, -1, 2, 2, 1, -1, 1, 2, 2]\n",
    "w11,w12,b1,w21,w22,b2,w31,w32,b3 = param\n",
    "netz = Netz(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n1 0 0\n",
      "n2 0 0\n",
      "n3 0.8807970779778823 0.2689414213699951\n",
      "0.9682832559919501\n"
     ]
    }
   ],
   "source": [
    "a = netz.forward(0,0)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030710792157577743\n"
     ]
    }
   ],
   "source": [
    "print(a*(1-a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/'> Beispiel nachv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
