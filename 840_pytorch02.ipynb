{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Perceptron mit einem Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([3.], requires_grad=True), Parameter containing:\n",
      "tensor([2.], requires_grad=True)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[8.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = nn.Linear(1,1)\n",
    "p.weight.data = torch.tensor([3.0])\n",
    "p.bias.data = torch.tensor([2.0])\n",
    "x = torch.FloatTensor([[[2.0]]])\n",
    "#x = torch.tensor([2.0])\n",
    "print(list(p.parameters()))\n",
    "p(x)                          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Perceptron mit zwei Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([3., 2.], requires_grad=True), Parameter containing:\n",
      "tensor([1.], requires_grad=True)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[19.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = nn.Linear(2,1)\n",
    "p.weight.data = torch.tensor([3.0, 2.0])\n",
    "p.bias.data = torch.tensor([1.0])\n",
    "print(list(p.parameters()))\n",
    "x = torch.FloatTensor([[[2.0,6.0]]])\n",
    "p(x)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Der Gradient\n",
    "\n",
    "#### Beispiel 1:\n",
    "\n",
    "Bestimme den Wert der Funktion Funktion $f(x_1,x_2,x_3) = (x_1^2 + 3x_2) \\cdot 2x_3$ am Punkt $x = (2,-1,3)$.\n",
    "Gehe von $x$ aus um $0.01$ Längeneinheiten in Richtung des negativen Gradienten zum Punkt $x'$ und werte dort die Funktion erneut aus.\n",
    "\n",
    "<img src=\"./img/nn_19.png\" width=\"800\"/>    \n",
    "\n",
    "\n",
    "Der gesuchte Gradient ist $(24, 18, 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6., grad_fn=<MulBackward0>)\n",
      "a.grad tensor(6.)\n",
      "6.0\n",
      "tensor(1.)\n",
      "tensor(6.)\n",
      "tensor(1.)\n",
      "tensor(24.)\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0,-1.0,3.0],requires_grad=True)\n",
    "x1,x2,x3 = x[0],x[1],x[2]\n",
    "a = x1 * x1\n",
    "b = 3 * x2\n",
    "c = 2 * x3\n",
    "d = a + b\n",
    "e = c * d\n",
    "\n",
    "a.retain_grad()\n",
    "b.retain_grad()\n",
    "c.retain_grad()\n",
    "d.retain_grad()\n",
    "e.retain_grad()\n",
    "x1.retain_grad()\n",
    "\n",
    "e.backward()\n",
    "\n",
    "print(e)\n",
    "\n",
    "print(\"a.grad\",a.grad)\n",
    "print(b.grad.numpy())\n",
    "print(c.grad)\n",
    "print(d.grad)\n",
    "print(e.grad)\n",
    "print(x1.grad)\n",
    "print(x1.grad/a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gegebener Punkt:  [ 2. -1.  3.]\n",
      "Auswertung der Funktion: 6.0\n",
      "Gradient:  [24. 18.  2.]\n",
      "Länge des Gradienten: 30.066593\n",
      "normierter negativer Gradient [-0.79822814 -0.5986711  -0.06651901]\n",
      "neuer Punkt:  [ 1.9920177 -1.0059867  2.9993348]\n",
      "Auswertung der Funktion: 5.6997833\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0,-1.0,3.0],requires_grad=True)\n",
    " \n",
    "y = (x[0]*x[0] + 3*x[1])*2*x[2]\n",
    " \n",
    "y.backward()\n",
    "\n",
    "x_given = x.detach().numpy()\n",
    "print(\"gegebener Punkt: \",x_given)\n",
    "print(\"Auswertung der Funktion:\", y.detach().numpy())\n",
    "\n",
    "grad = x.grad.numpy()\n",
    "print(\"Gradient: \",grad)\n",
    "\n",
    "gradLength = np.linalg.norm(grad)   \n",
    "print(\"Länge des Gradienten:\", gradLength )\n",
    "\n",
    "descent = -grad/gradLength\n",
    "print(\"normierter negativer Gradient\",  descent)\n",
    "\n",
    "learning_rate = 0.01\n",
    "x_new = x.detach().numpy() + learning_rate * descent\n",
    "print(\"neuer Punkt: \",x_new)\n",
    "\n",
    "x = Variable(torch.FloatTensor(x_new),requires_grad=True)\n",
    "y = (x[0]*x[0] + 3*x[1])*2*x[2]\n",
    " \n",
    "print(\"Auswertung der Funktion:\", y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2., -1.,  3.])\n",
      "[ 2. -1.  3.]\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0,-1.0,3.0])\n",
    "print(x)\n",
    " \n",
    "print(x1.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2., -1.,  3.]) [ 2. -1.  3.]\n"
     ]
    }
   ],
   "source": [
    "# Einen Tensor erstellen, für den man keinen Gradienten braucht, z.B. für die Daten\n",
    "x = torch.tensor([2.0,-1.0,3.0])   # requires_grad=False ist default\n",
    "xn = x.numpy()\n",
    "print(x, xn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2., -1.,  3.], requires_grad=True) tensor([ 2., -1.,  3.]) [ 2. -1.  3.]\n"
     ]
    }
   ],
   "source": [
    "# Einen Tensor erstellen, für den man einen Gradienten benötigt.\n",
    "x = torch.tensor([2.0,-1.0,3.0],requires_grad=True)   # requires_grad=False ist default\n",
    "xd = x.detach()       # ohne Gradient\n",
    "xn = xd.numpy()\n",
    "print(x, xd, xn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gegebener Punkt:  [ 2. -1.  3.]\n",
      "Auswertung der Funktion: 6.0\n",
      "Gradient:  [24. 18.  2.]\n",
      "Länge des Gradienten: 30.066593\n",
      "normierter negativer Gradient [-0.79822814 -0.5986711  -0.06651901]\n",
      "neuer Punkt:  [ 1.9920177 -1.0059867  2.9993348]\n",
      "Auswertung der Funktion: 5.6997833\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = Variable(torch.FloatTensor([2.0,-1.0,3.0]),requires_grad=True)\n",
    "y = (x[0]*x[0] + 3*x[1])*2*x[2]\n",
    " \n",
    "y.backward()\n",
    "\n",
    "x_given = x.detach().numpy()\n",
    "print(\"gegebener Punkt: \",x_given)\n",
    "print(\"Auswertung der Funktion:\", y.detach().numpy())\n",
    "\n",
    "grad = x.grad.numpy()\n",
    "print(\"Gradient: \",grad)\n",
    "\n",
    "gradLength = np.linalg.norm(grad)   \n",
    "print(\"Länge des Gradienten:\", gradLength )\n",
    "\n",
    "descent = -grad/gradLength\n",
    "print(\"normierter negativer Gradient\",  descent)\n",
    "\n",
    "learning_rate = 0.01\n",
    "x_new = x.detach().numpy() + learning_rate * descent\n",
    "print(\"neuer Punkt: \",x_new)\n",
    "\n",
    "x = Variable(torch.FloatTensor(x_new),requires_grad=True)\n",
    "y = (x[0]*x[0] + 3*x[1])*2*x[2]\n",
    " \n",
    "print(\"Auswertung der Funktion:\", y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel 2:\n",
    "Bestimme den Wert der Funktion $f(x_1,x_2,x_3) = (x_1\\cdot x_2 + x_1^2) \\cdot (x_3 - 5)$ am Punkt $x = (1,-2,2)$.\n",
    "Gehe von $x$ aus um $0.01$ Längeneinheiten in Richtung des negativen Gradienten zum Punkt $x'$ und werte dort die Funktion erneut aus.\n",
    "(Mit etwas Übung kann man bei einfachen Funktionen die partiellen Ableitungen direkt hinschreiben). \n",
    "\n",
    "<img src=\"./img/nn_20.png\" width=\"800\"/>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gegebener Punkt:  [ 1. -2.  2.]\n",
      "Auswertung der Funktion: 3.0\n",
      "Gradient:  [ 0. -3. -1.]\n",
      "Länge des Gradienten: 3.1622777\n",
      "normierter negativer Gradient [-0.          0.94868326  0.31622776]\n",
      "neuer Punkt:  [ 1.9920177 -1.0059867  2.9993348]\n",
      "Auswertung der Funktion: 2.9684072\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = Variable(torch.FloatTensor([1.0,-2.0,2.0]),requires_grad=True)\n",
    "y = (x[0]*x[1] + x[0] * x[0])*(x[2]-5)\n",
    " \n",
    "y.backward()\n",
    "\n",
    "x_given = x.detach().numpy()\n",
    "print(\"gegebener Punkt: \",x_given)\n",
    "print(\"Auswertung der Funktion:\", y.detach().numpy())\n",
    "\n",
    "grad = x.grad.numpy()\n",
    "print(\"Gradient: \",grad)\n",
    "\n",
    "gradLength = np.linalg.norm(grad)   \n",
    "print(\"Länge des Gradienten:\", gradLength )\n",
    "\n",
    "descent = -grad/gradLength\n",
    "print(\"normierter negativer Gradient\",  descent)\n",
    "\n",
    "learning_rate = 0.01\n",
    "x_new = x.detach().numpy() + learning_rate * descent\n",
    "print(\"neuer Punkt: \",new_x)\n",
    "\n",
    "x = Variable(torch.FloatTensor(x_new),requires_grad=True)\n",
    "y = (x[0]*x[1] + x[0] * x[0])*(x[2]-5)\n",
    " \n",
    "print(\"Auswertung der Funktion:\", y.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Daten soll das richtige Gewicht gefunden werden:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 7.992253303527832\n",
      "Epoch 1 - loss: 2.235430955886841\n",
      "Epoch 2 - loss: 0.385786771774292\n",
      "Epoch 3 - loss: 0.7099998593330383\n",
      "Epoch 4 - loss: 0.5065121054649353\n",
      "Epoch 5 - loss: 0.4985172748565674\n",
      "Epoch 6 - loss: 0.4343971610069275\n",
      "Epoch 7 - loss: 0.3958275020122528\n",
      "Epoch 8 - loss: 0.3547412157058716\n",
      "Epoch 9 - loss: 0.3198792636394501\n",
      "Epoch 10 - loss: 0.28778746724128723\n",
      "Epoch 11 - loss: 0.2591332495212555\n",
      "Epoch 12 - loss: 0.23325933516025543\n",
      "Epoch 13 - loss: 0.20999310910701752\n",
      "Epoch 14 - loss: 0.18903934955596924\n",
      "Epoch 15 - loss: 0.17017938196659088\n",
      "Epoch 16 - loss: 0.1532001942396164\n",
      "Epoch 17 - loss: 0.13791537284851074\n",
      "Epoch 18 - loss: 0.12415505945682526\n",
      "Epoch 19 - loss: 0.11176783591508865\n",
      "Epoch 20 - loss: 0.10061671584844589\n",
      "Epoch 21 - loss: 0.09057796746492386\n",
      "Epoch 22 - loss: 0.0815407782793045\n",
      "Epoch 23 - loss: 0.07340516149997711\n",
      "Epoch 24 - loss: 0.0660814642906189\n",
      "Epoch 25 - loss: 0.05948852002620697\n",
      "Epoch 26 - loss: 0.053553249686956406\n",
      "Epoch 27 - loss: 0.048210009932518005\n",
      "Epoch 28 - loss: 0.04340016096830368\n",
      "Epoch 29 - loss: 0.03906993567943573\n",
      "Epoch 30 - loss: 0.0351719856262207\n",
      "Epoch 31 - loss: 0.031662605702877045\n",
      "Epoch 32 - loss: 0.028503620997071266\n",
      "Epoch 33 - loss: 0.025659723207354546\n",
      "Epoch 34 - loss: 0.023099636659026146\n",
      "Epoch 35 - loss: 0.02079497091472149\n",
      "Epoch 36 - loss: 0.01872025616466999\n",
      "Epoch 37 - loss: 0.016852518543601036\n",
      "Epoch 38 - loss: 0.015170999802649021\n",
      "Epoch 39 - loss: 0.01365746557712555\n",
      "Epoch 40 - loss: 0.012294774875044823\n",
      "Epoch 41 - loss: 0.011068054474890232\n",
      "Epoch 42 - loss: 0.009963869117200375\n",
      "Epoch 43 - loss: 0.008969688788056374\n",
      "Epoch 44 - loss: 0.008074812591075897\n",
      "Epoch 45 - loss: 0.0072691310197114944\n",
      "Epoch 46 - loss: 0.006543915253132582\n",
      "Epoch 47 - loss: 0.005891044624149799\n",
      "Epoch 48 - loss: 0.005303300451487303\n",
      "Epoch 49 - loss: 0.004774104803800583\n",
      "Epoch 50 - loss: 0.0042977831326425076\n",
      "Epoch 51 - loss: 0.003869056235998869\n",
      "Epoch 52 - loss: 0.003483027685433626\n",
      "Epoch 53 - loss: 0.003135465318337083\n",
      "Epoch 54 - loss: 0.002822686219587922\n",
      "Epoch 55 - loss: 0.002541027031838894\n",
      "Epoch 56 - loss: 0.0022875135764479637\n",
      "Epoch 57 - loss: 0.0020592682994902134\n",
      "Epoch 58 - loss: 0.0018538200529292226\n",
      "Epoch 59 - loss: 0.0016688526375219226\n",
      "Epoch 60 - loss: 0.001502370461821556\n",
      "Epoch 61 - loss: 0.0013524615205824375\n",
      "Epoch 62 - loss: 0.0012175239389762282\n",
      "Epoch 63 - loss: 0.0010960446670651436\n",
      "Epoch 64 - loss: 0.0009867065818980336\n",
      "Epoch 65 - loss: 0.0008882210240699351\n",
      "Epoch 66 - loss: 0.000799638160970062\n",
      "Epoch 67 - loss: 0.0007198466337285936\n",
      "Epoch 68 - loss: 0.0006480176234617829\n",
      "Epoch 69 - loss: 0.0005833671311847866\n",
      "Epoch 70 - loss: 0.0005251794354990125\n",
      "Epoch 71 - loss: 0.0004727710038423538\n",
      "Epoch 72 - loss: 0.0004256019019521773\n",
      "Epoch 73 - loss: 0.00038312928518280387\n",
      "Epoch 74 - loss: 0.00034490489633753896\n",
      "Epoch 75 - loss: 0.00031050105462782085\n",
      "Epoch 76 - loss: 0.0002795204345602542\n",
      "Epoch 77 - loss: 0.00025163291138596833\n",
      "Epoch 78 - loss: 0.00022652205370832235\n",
      "Epoch 79 - loss: 0.00020393432350829244\n",
      "Epoch 80 - loss: 0.0001835778239183128\n",
      "Epoch 81 - loss: 0.00016525851970072836\n",
      "Epoch 82 - loss: 0.00014877306239213794\n",
      "Epoch 83 - loss: 0.00013393057452049106\n",
      "Epoch 84 - loss: 0.00012056323612341657\n",
      "Epoch 85 - loss: 0.0001085433759726584\n",
      "Epoch 86 - loss: 9.77099480223842e-05\n",
      "Epoch 87 - loss: 8.79639555932954e-05\n",
      "Epoch 88 - loss: 7.918324263300747e-05\n",
      "Epoch 89 - loss: 7.128220022423193e-05\n",
      "Epoch 90 - loss: 6.417394615709782e-05\n",
      "Epoch 91 - loss: 5.777554542873986e-05\n",
      "Epoch 92 - loss: 5.200840314500965e-05\n",
      "Epoch 93 - loss: 4.681809150497429e-05\n",
      "Epoch 94 - loss: 4.214785440126434e-05\n",
      "Epoch 95 - loss: 3.7942918424960226e-05\n",
      "Epoch 96 - loss: 3.4156480978708714e-05\n",
      "Epoch 97 - loss: 3.074857158935629e-05\n",
      "Epoch 98 - loss: 2.7680043785949238e-05\n",
      "Epoch 99 - loss: 2.4917766495491378e-05\n",
      "[Parameter containing:\n",
      "tensor([[2.9990]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0058], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "data = [(1,3), (2,6), (3,9), (4,12), (5,15), (6,18)]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1,1)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "def criterion(out, label):\n",
    "    return (label - out)**2\n",
    "\n",
    "net = Net()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, data2 in enumerate(data):\n",
    "        X, Y = iter(data2)\n",
    "        X, Y = Variable(torch.FloatTensor([X]), requires_grad=True),Variable(torch.FloatTensor([Y]), requires_grad=False)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i % 10 == 0):\n",
    "            print(\"Epoch {} - loss: {}\".format(epoch, loss.data[0])) \n",
    "            \n",
    "print(list(net.parameters()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3\n",
      "2 6\n",
      "3 9\n",
      "4 12\n",
      "5 15\n",
      "6 18\n"
     ]
    }
   ],
   "source": [
    "data = [(1,3), (2,6), (3,9), (4,12), (5,15), (6,18)]\n",
    "for i, data2 in enumerate(data):\n",
    "    X, Y = data2\n",
    "    print(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4., grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2., 3.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor([-1.0,2.0]),requires_grad=True)\n",
    "y = 2*x[0] + 3*x[1]\n",
    "x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<PowBackward0>) tensor([2.], grad_fn=<MulBackward0>) tensor([8.], grad_fn=<PowBackward0>) tensor([2.], grad_fn=<MulBackward0>) tensor([4.], grad_fn=<MulBackward0>)\n",
      "tensor([48.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(([1.0]),requires_grad=True)\n",
    "y = x**2\n",
    "z = 2*y\n",
    "w= z**3\n",
    "\n",
    "# This is the subpath\n",
    "# Do not use detach()\n",
    "p = z\n",
    "q = torch.tensor(([2.0]), requires_grad=True)\n",
    "pq = p*q\n",
    "#pq.backward(retain_graph=True)\n",
    "\n",
    "w.backward()\n",
    "print(y,z,w,p,pq)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
