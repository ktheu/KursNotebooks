{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        # hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # hidden linear layer\n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1)\n",
    "\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir erstellen das train_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Der Forward-Pass für ein Bild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 9)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label  = next(iter(train_set)) \n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein einzelnes Bild können wir als Batch der Länge 1 auffassen. Wir bringen den Eingabe-Tensor in das gewünschte Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0718,  0.1010, -0.0319, -0.0910, -0.0869,  0.0176, -0.0334, -0.1134, -0.1003,  0.0897]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network()\n",
    "pred = net(image.unsqueeze(0))\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Softmax-Funktion normiert die letzte Ausgabe so, dass sie als Wahrscheinlichkeiten gelesen werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1768, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir suchen den Index mit der größten Wahrscheinlichkeit und vergleichen ihn mit dem Label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.argmax().item(), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Der Forward-Pass für einen Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1, 28, 28]), tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set\n",
    "    ,batch_size=10\n",
    ")\n",
    "batch = iter(train_loader)\n",
    "images, labels = next(batch)\n",
    "images.shape, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 10]),\n",
       " tensor([[ 0.0718,  0.1010, -0.0319, -0.0910, -0.0869,  0.0176, -0.0334, -0.1134, -0.1003,  0.0897],\n",
       "         [ 0.0567,  0.0951, -0.0375, -0.0988, -0.0845,  0.0174, -0.0302, -0.1035, -0.1008,  0.0916],\n",
       "         [ 0.0576,  0.0953, -0.0428, -0.0939, -0.0824, -0.0052, -0.0239, -0.1076, -0.1023,  0.0861],\n",
       "         [ 0.0597,  0.0954, -0.0412, -0.0922, -0.0847, -0.0033, -0.0301, -0.1139, -0.1034,  0.0877],\n",
       "         [ 0.0642,  0.0942, -0.0353, -0.0911, -0.0879,  0.0011, -0.0394, -0.1177, -0.1016,  0.0948],\n",
       "         [ 0.0608,  0.0975, -0.0333, -0.0949, -0.0858,  0.0178, -0.0335, -0.1094, -0.1031,  0.0868],\n",
       "         [ 0.0526,  0.0952, -0.0358, -0.0988, -0.0828,  0.0052, -0.0289, -0.1116, -0.1027,  0.0829],\n",
       "         [ 0.0640,  0.0950, -0.0340, -0.0949, -0.0791,  0.0186, -0.0278, -0.1034, -0.1104,  0.0817],\n",
       "         [ 0.0650,  0.0970, -0.0329, -0.0897, -0.0794,  0.0019, -0.0256, -0.1048, -0.0971,  0.0857],\n",
       "         [ 0.0665,  0.0943, -0.0272, -0.1087, -0.0864,  0.0335, -0.0307, -0.1033, -0.1064,  0.0884]],\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = net(images)\n",
    "preds.shape, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir vergleichen die Vorhersage mit den Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 1, 1, 9, 1, 1, 1, 1, 1]),\n",
       " tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wo stimmen die Werte überein?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1).eq(labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anzahl der Übereinstimmungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1).eq(labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "get_num_correct(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss, Gradient, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3034, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(preds, labels) \n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-762677ec1f99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(net.conv1.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n",
      "tensor([[[[ 1.5791e-06,  3.0987e-06, -2.3689e-06, -2.9477e-06, -1.2152e-05],\n",
      "          [ 1.2617e-08, -1.6410e-07, -4.5328e-06, -1.6404e-05, -9.0226e-06],\n",
      "          [-6.5153e-07, -8.2259e-07, -1.5639e-05, -1.5979e-05,  3.9226e-06],\n",
      "          [-5.3651e-07, -5.4110e-06, -2.0092e-05, -1.6125e-06,  2.3402e-06],\n",
      "          [-1.1005e-06, -1.2314e-05, -1.6940e-05,  3.5824e-07, -4.3728e-06]]],\n",
      "\n",
      "\n",
      "        [[[-5.9894e-05, -1.2090e-04, -5.0924e-05,  9.6953e-06, -1.4242e-04],\n",
      "          [-3.0523e-05, -2.1103e-05, -4.2214e-05,  2.8891e-05, -1.1105e-04],\n",
      "          [ 6.7964e-05,  7.7582e-05,  5.8374e-05,  2.7225e-06, -2.1703e-04],\n",
      "          [ 1.4244e-04,  6.1349e-05,  1.6056e-05, -2.6335e-05, -2.6858e-04],\n",
      "          [ 1.6518e-04,  1.5072e-04, -1.6581e-05,  3.6636e-06, -2.2057e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0203e-07,  2.9694e-07,  8.1515e-07, -1.7188e-08,  2.3142e-07],\n",
      "          [ 3.0464e-08,  3.0464e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 1.0385e-06,  2.5008e-06,  3.8924e-06,  9.0588e-06,  1.2330e-05],\n",
      "          [ 1.4115e-05,  1.7077e-05,  2.9294e-05,  1.8422e-05,  2.1223e-05],\n",
      "          [ 4.4477e-06,  4.9961e-06,  4.5430e-06,  3.7466e-07, -1.1463e-07]]],\n",
      "\n",
      "\n",
      "        [[[-6.2033e-05,  3.6157e-05,  1.5188e-05, -5.2598e-05, -9.4431e-05],\n",
      "          [ 4.6313e-05,  6.7413e-05,  1.0939e-04, -8.3723e-06, -1.1511e-04],\n",
      "          [ 1.5013e-04,  1.5551e-04,  2.2535e-04,  9.5226e-05,  1.9273e-05],\n",
      "          [ 1.0520e-04,  1.0911e-04,  1.2081e-04,  7.7237e-05, -5.2760e-06],\n",
      "          [ 4.3332e-05,  1.0122e-04,  1.2295e-04,  6.3332e-05, -4.4838e-05]]],\n",
      "\n",
      "\n",
      "        [[[-1.9101e-04, -1.9007e-04, -1.5228e-04, -1.5775e-04, -3.1637e-04],\n",
      "          [-1.5811e-04, -1.6035e-04, -1.8698e-04, -2.0076e-04, -2.9700e-04],\n",
      "          [-3.0270e-04, -2.6220e-04, -3.5961e-04, -3.5071e-04, -3.5159e-04],\n",
      "          [-2.2060e-04, -2.2885e-04, -2.7463e-04, -3.1152e-04, -2.3848e-04],\n",
      "          [-1.9931e-04, -1.7045e-04, -1.5116e-04, -1.9779e-04, -1.1366e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 2.2900e-04,  2.4133e-04,  2.9327e-04,  2.1409e-04,  1.8771e-04],\n",
      "          [ 2.2984e-04,  1.5213e-04,  7.9144e-05,  4.9498e-05,  2.0044e-04],\n",
      "          [ 1.4522e-04,  1.5594e-05,  6.2642e-05,  5.2203e-05,  1.3546e-04],\n",
      "          [-1.5965e-05,  4.1567e-05,  1.3518e-04,  1.1786e-04,  8.1839e-05],\n",
      "          [ 4.9799e-06,  6.8010e-05,  1.4287e-04,  1.4172e-04,  1.4241e-04]]]])\n"
     ]
    }
   ],
   "source": [
    "print(net.conv1.weight.grad.shape)\n",
    "print(net.conv1.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wählen einen Optimizer und ihn die Gewichte updaten. Danach machen wir wieder eine prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.1882872581481934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "optimizer.step()  \n",
    "preds = net(images)\n",
    "\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "print('loss:', loss.item())\n",
    "get_num_correct(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dreimal der gesamte Ablauf mit einer Batch-Size von 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 2.302917003631592 num correct: 8\n",
      "loss2: 2.299283027648926 num correct: 15\n",
      "loss3: 2.28469181060791 num correct: 15\n"
     ]
    }
   ],
   "source": [
    "net = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "batch = next(iter(train_loader)) # Get Batch\n",
    "images, labels = batch\n",
    "\n",
    "preds = net(images)  \n",
    "loss = F.cross_entropy(preds, labels)  \n",
    "print('loss1:', loss.item(), 'num correct:', get_num_correct(preds,labels))\n",
    "\n",
    "loss.backward()  \n",
    "optimizer.step()  \n",
    "preds = net(images)\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "print('loss2:', loss.item(), 'num correct:', get_num_correct(preds,labels))\n",
    "\n",
    "loss.backward()  \n",
    "optimizer.step()  \n",
    "preds = net(images)\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "print('loss3:', loss.item(), 'num correct:', get_num_correct(preds,labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eine Epoche\n",
    "\n",
    "Wenn die Batche alle Daten des Trainingssets durchlaufen haben, ist eine *Epoche* zu Ende."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct: 47014 loss: 342.30467838048935\n"
     ]
    }
   ],
   "source": [
    "net = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "\n",
    "for batch in train_loader: # Get Batch\n",
    "    images, labels = batch \n",
    "\n",
    "    preds = net(images) # Pass Batch\n",
    "    loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() # Calculate Gradients\n",
    "    optimizer.step() # Update Weights\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    total_correct += get_num_correct(preds, labels)\n",
    "    \n",
    "print(\n",
    "    \"epoch:\", 0, \n",
    "    \"total_correct:\", total_correct, \n",
    "    \"loss:\", total_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prozentsatz der richtigen Zuordnungen nach einer Epoche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7835666666666666"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_correct / len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training mit mehreren Epochen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_correct: 46886 loss: 348.9371727705002\n",
      "epoch 1 total_correct: 51228 loss: 240.15532341599464\n",
      "epoch 2 total_correct: 52097 loss: 215.706786647439\n",
      "epoch 3 total_correct: 52549 loss: 204.76445497572422\n",
      "epoch 4 total_correct: 52653 loss: 198.21014676988125\n",
      "epoch 5 total_correct: 52711 loss: 196.00613040477037\n",
      "epoch 6 total_correct: 52696 loss: 198.06398537755013\n",
      "epoch 7 total_correct: 53015 loss: 190.6323289424181\n",
      "epoch 8 total_correct: 53302 loss: 181.53220679610968\n",
      "epoch 9 total_correct: 53201 loss: 184.2184516787529\n"
     ]
    }
   ],
   "source": [
    "net = Network()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "     train_set\n",
    "    ,batch_size=100\n",
    "    ,shuffle=True\n",
    ")\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch in train_loader: # Get Batch\n",
    "        images, labels = batch \n",
    "\n",
    "        preds = net(images) # Pass Batch\n",
    "        loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # Calculate Gradients\n",
    "        optimizer.step() # Update Weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "    print(\n",
    "        \"epoch\", epoch, \n",
    "        \"total_correct:\", total_correct, \n",
    "        \"loss:\", total_loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "epoch 0 total_correct: 46886 loss: 348.9371727705002\n",
    "epoch 1 total_correct: 51228 loss: 240.15532341599464\n",
    "epoch 2 total_correct: 52097 loss: 215.706786647439\n",
    "epoch 3 total_correct: 52549 loss: 204.76445497572422\n",
    "epoch 4 total_correct: 52653 loss: 198.21014676988125\n",
    "epoch 5 total_correct: 52711 loss: 196.00613040477037\n",
    "epoch 6 total_correct: 52696 loss: 198.06398537755013\n",
    "epoch 7 total_correct: 53015 loss: 190.6323289424181\n",
    "epoch 8 total_correct: 53302 loss: 181.53220679610968\n",
    "epoch 9 total_correct: 53201 loss: 184.2184516787529\n",
    "correct- test: correct: 0.8747\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anwenden auf die Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 0.8747\n"
     ]
    }
   ],
   "source": [
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data'\n",
    "    ,train=False\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set\n",
    "    ,batch_size=100\n",
    ")\n",
    "\n",
    "correct = 0\n",
    "for batch in test_loader:\n",
    "    images, labels = batch \n",
    "    preds = net(images)\n",
    "    \n",
    "    images.shape, labels\n",
    "    correct += get_num_correct(preds,labels)\n",
    "    \n",
    "print(\"correct:\",correct/len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model und Parameter speichern und laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khthe\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Network. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\khthe\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\khthe\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "checkpoint = {'model': Network(),\n",
    "          'state_dict': net.state_dict(),\n",
    "          'optimizer' : optimizer.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = checkpoint['model']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = load_checkpoint('checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "\n",
    "batch = next(iter(train_loader)) # Get Batch\n",
    "images, labels = batch\n",
    "preds = model(images) # Pass Batch\n",
    "get_num_correct(preds, labels)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
