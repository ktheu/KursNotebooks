{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Softmax-Funktion\n",
    "\n",
    "Die Sigmoid-Aktivierung nutzen wir, wenn wir zwei Klassen unterscheiden wollen (0,1). Die Softmax-Funktion nutzen wir als Aktivierungsfunktion bei mehr als 2 Klassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wird ein neuronales Netz zur Klassifikation benutzt wird, dann nennt man den letzten Layer häufig den\n",
    "**logits-layer**. Er produziert einen Vektor aus reellen Zahlen zwischen minus unendlich und plus unendlich, für jede Klassifikationklasse eine Zahl. Als Aktivierungsfunktion wird häufig die **softmax** -Funktion benutzt. Die Aufgabe der Softmax-Funktion ist es, den Vektor umzuwandeln in einen Vektor aus Wahrscheinlichkeiten dafür, dass die jeweilige Klasse erkannt wurde.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da Wahrscheinlichkeiten immer positive Zahlen sind, werden die logits zunächst in positive Zahlen gewandelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.3890561 , 0.36787944, 1.10517092])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = [2, -1, 0.1]\n",
    "np.exp(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann werden die Zahlen normiert, damit ihre Summe 1 ergibt. Dies ist die Ausgabe der Softmax-Funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83378101, 0.04151151, 0.12470747])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.exp(logits)\n",
    "a = a / np.sum(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.sum(torch.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8338, 0.0415, 0.1247])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(torch.tensor([2,-1,0.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1818)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor([1])\n",
    "x = torch.tensor([[2,-1,0.1]])\n",
    "loss = F.cross_entropy(x, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8338, 0.0415, 0.1247]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = softmax(x)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1818, 3.1818, 2.0818]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- torch.log(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1818)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor([1,1])\n",
    "x = torch.tensor([[2,-1,0.1],[2,-1,0.1]])\n",
    "loss = F.cross_entropy(x, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8338, 0.0415, 0.1247],\n",
       "        [0.8338, 0.0415, 0.1247]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = F.torch.softmax(x,dim=1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1818, 3.1818, 2.0818],\n",
       "        [0.1818, 3.1818, 2.0818]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6624)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor([1,2])\n",
    "x = torch.tensor([[2,-1,0.1],[3,-1,1]])\n",
    "loss = F.cross_entropy(x, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8338, 0.0415, 0.1247],\n",
       "        [0.8668, 0.0159, 0.1173]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = F.torch.softmax(x,dim=1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1818, 3.1818, 2.0818],\n",
       "        [0.1429, 4.1429, 2.1429]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.66235"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3.1818+2.1429)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
