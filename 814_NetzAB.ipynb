{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NetzA\n",
    "\n",
    "<img src=\"./img/netzA.png\" width=\"800\">\n",
    "\n",
    "<img src=\"./img/netzAa.png\" width=\"601\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class NetzA:\n",
    "    def __init__(self,param):\n",
    "        self.param = param\n",
    "   \n",
    "    def calc(self, x1, x2, y, lr):\n",
    "        w1, w2, b = self.param   \n",
    "        # forward\n",
    "        z = w1 * x1 + w2 * x2 + b   \n",
    "        a = 1/(1+math.exp(-1.0*z))\n",
    "        y_hat = a\n",
    "        \n",
    "        print(\"\\nParameter:\")\n",
    "        print(\"w1={:6.4f}, w2={:6.4f}, b={:6.4f}\".format(w1,w2,b))\n",
    "        print(\"z={:6.4f}, a={:6.4f}\".format(z,a))\n",
    "        \n",
    "        # backward\n",
    "        db = (y_hat - y)*a*(1-a)    \n",
    "        dw1 = x1 * db\n",
    "        dw2 = x2 * db\n",
    "        \n",
    "        # optimize\n",
    "        w1 -= lr * dw1               \n",
    "        w2 -= lr * dw2\n",
    "        b  -= lr * db\n",
    "                \n",
    "        print(\"\\nGradienten:\")\n",
    "        print(\"dw1={:6.4f}, dw2={:6.4f}, db={:6.4f}\".format(dw1,dw2,db))\n",
    "         \n",
    "        print(\"\\nUpdated Parameter, learningrate =\", lr)\n",
    "        print(\"w1={:6.4f}, w2={:6.4f}, b={:6.4f}\".format(w1,w2,b))\n",
    "        print(\"----------------\")\n",
    "        \n",
    "        self.param = [w1, w2, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameter:\n",
      "w1=-1.0000, w2=1.0000, b=-1.5000\n",
      "z=-2.5000, a=0.0759\n",
      "\n",
      "Gradienten:\n",
      "dw1=-0.0648, dw2=-0.0000, db=-0.0648\n",
      "\n",
      "Updated Parameter, learningrate = 0.1\n",
      "w1=-0.9935, w2=1.0000, b=-1.4935\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "param = [-1, 1, -1.5]\n",
    "netz = NetzA(param)\n",
    "netz.calc(1,0,y=1,lr=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NetzA mit Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameter:\n",
      "Parameter containing:\n",
      "tensor([-1.,  1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.5000], requires_grad=True)\n",
      "\n",
      "Gradienten:\n",
      "tensor([-0.0648,  0.0000])\n",
      "tensor([-0.0648])\n",
      "\n",
      "Updated Parameter, learningrate = 0.1\n",
      "Parameter containing:\n",
      "tensor([-0.9935,  1.0000], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.4935], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class NetA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2,1)\n",
    " \n",
    "    def forward(self, t):\n",
    "        t = self.fc1(t)\n",
    "        t = torch.sigmoid(t)\n",
    "        return t\n",
    "\n",
    "def loss_fn(y, y_hat):\n",
    "    return 0.5*(y-y_hat)**2\n",
    "\n",
    "w1, w2, b = [-1., 1., -1.5]\n",
    "\n",
    "net = NetA()\n",
    "net.fc1.weight.data = torch.tensor([w1, w2]) \n",
    "net.fc1.bias.data = torch.tensor([b]) \n",
    "\n",
    "y = torch.tensor([1]).reshape(1,1)\n",
    "x = torch.tensor([1.0,0]).reshape(1,1,2)\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "print(\"\\nParameter:\")\n",
    "for p in net.parameters():\n",
    "    print(p)\n",
    "    \n",
    "loss = loss_fn(net(x), y)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nGradienten:\")\n",
    "for p in net.parameters():\n",
    "    print(p.grad)\n",
    "\n",
    "optimizer.step()\n",
    "print(\"\\nUpdated Parameter, learningrate =\", lr)\n",
    "for p in net.parameters():\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9935,  1.0000])\n"
     ]
    }
   ],
   "source": [
    "loss.type()\n",
    "print(net.fc1.weight.data)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NetzB\n",
    "\n",
    "<img src=\"./img/netzB.png\" width=\"900\">\n",
    "\n",
    "<img src=\"./img/netzBa.png\" width=\"701\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class NetzB:\n",
    "    def __init__(self,param):\n",
    "        self.param = param\n",
    "   \n",
    "    def calc(self, x1, x2, y, lr):\n",
    "        w11, w12, b1, w21, w22, b2, w31, w32, b3 = self.param\n",
    "        \n",
    "        # forward\n",
    "        z1 = w11 * x1 + w12 * x2 + b1      \n",
    "        a1 = z1 if z1 > 0 else 0 \n",
    "        \n",
    "        z2 = w21 * x1 + w22 * x2 + b2 \n",
    "        a2 = z2 if z2 > 0 else 0 \n",
    "        \n",
    "        z3 = w31 * a1 + w32 * a2 + b3\n",
    "        a3 = 1/(1+math.exp(-1.0*z3))\n",
    "        y_hat = a3\n",
    "        \n",
    "        print(\"\\nForward:\")\n",
    "        print(\"w11={:6.4f}, w12={:6.4f}, b1={:6.4f}\".format(w11,w12,b1))\n",
    "        print(\"w21={:6.4f}, w22={:6.4f}, b2={:6.4f}\".format(w21,w22,b2))\n",
    "        print(\"w31={:6.4f}, w32={:6.4f}, b3={:6.4f}\".format(w31,w32,b3))\n",
    "        \n",
    "        print(\"z1={:6.4f}, a1={:6.4f}\".format(z1,a1))\n",
    "        print(\"z2={:6.4f}, a2={:6.4f}\".format(z2,a2))\n",
    "        print(\"z3={:6.4f}, a3={:6.4f}\".format(z3,a3))\n",
    "        \n",
    "        # backward\n",
    "        db3 = (y_hat-y)*a3*(1-a3)              \n",
    "        dw31 = a1 * db3\n",
    "        dw32 = a2 * db3\n",
    "        \n",
    "        if a1 > 0:\n",
    "            db1 = db3 * w31  \n",
    "            dw11 = db1 * x1 \n",
    "            dw12 = db1 * x2\n",
    "        else:\n",
    "            db1 = dw11 = dw12 = 0\n",
    "            \n",
    "        if a2 > 0:\n",
    "            db2 = db3 * w32  \n",
    "            dw21 = db2 * x1 \n",
    "            dw22 = db2 * x2\n",
    "        else:\n",
    "            db2 = dw21 = dw22 = 0\n",
    "           \n",
    "        # optimize\n",
    "        b1 -= lr * db1                    \n",
    "        w11 -= lr * dw11\n",
    "        w12 -= lr * dw12\n",
    "             \n",
    "        b2 -= lr * db2\n",
    "        w21 -= lr * dw21\n",
    "        w22 -= lr * dw22\n",
    "            \n",
    "        b3 -= lr * db3\n",
    "        w31 -= lr * dw31\n",
    "        w32 -= lr * dw32\n",
    "        \n",
    "        print(\"\\nGradienten:\")\n",
    "        print(\"dw11={:6.4f}, dw12={:6.4f}, db1={:6.4f}\".format(dw11,dw12,db1))\n",
    "        print(\"dw21={:6.4f}, dw22={:6.4f}, db2={:6.4f}\".format(dw21,dw22,db2))\n",
    "        print(\"dw31={:6.4f}, dw32={:6.4f}, db3={:6.4f}\".format(dw31,dw32,db3))\n",
    "\n",
    "        print(\"\\nUpdated Parameter:\")\n",
    "        print(\"w11={:6.4f}, w12={:6.4f}, b1={:6.4f}\".format(w11,w12,b1))\n",
    "        print(\"w21={:6.4f}, w22={:6.4f}, b2={:6.4f}\".format(w21,w22,b2))\n",
    "        print(\"w31={:6.4f}, w32={:6.4f}, b3={:6.4f}\".format(w31,w32,b3))\n",
    "    \n",
    "\n",
    "        \n",
    "        self.param = [w11, w12, b1, w21, w22, b2, w31, w32, b3]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forward:\n",
      "w11=-3.0000, w12=2.0000, b1=5.0000\n",
      "w21=3.0000, w22=-1.0000, b2=-1.0000\n",
      "w31=-1.0000, w32=5.0000, b3=-1.0000\n",
      "z1=6.0000, a1=6.0000\n",
      "z2=0.0000, a2=0.0000\n",
      "z3=-7.0000, a3=0.0009\n",
      "\n",
      "Gradienten:\n",
      "dw11=0.0009, dw12=0.0018, db1=0.0009\n",
      "dw21=0.0000, dw22=0.0000, db2=0.0000\n",
      "dw31=-0.0055, dw32=-0.0000, db3=-0.0009\n",
      "\n",
      "Updated Parameter:\n",
      "w11=-3.0003, w12=1.9995, b1=4.9997\n",
      "w21=3.0000, w22=-1.0000, b2=-1.0000\n",
      "w31=-0.9984, w32=5.0000, b3=-0.9997\n"
     ]
    }
   ],
   "source": [
    "param = [-3, 2, 5, 3, -1, -1, -1, 5, -1]\n",
    "netz = NetzB(param)\n",
    "netz.calc(1,2,y=1,lr=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NetzB mit Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ausgangslage:\n",
      "Parameter containing:\n",
      "tensor([[-3.,  2.],\n",
      "        [ 3., -1.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 5., -1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.,  5.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.], requires_grad=True)\n",
      "\n",
      "Gradienten:\n",
      "tensor([[0.0009, 0.0018],\n",
      "        [0.0000, 0.0000]])\n",
      "tensor([0.0009, 0.0000])\n",
      "tensor([-0.0055,  0.0000])\n",
      "tensor([-0.0009])\n",
      "\n",
      "Updated Parameter, learningrate = 0.3\n",
      "Parameter containing:\n",
      "tensor([[-3.0003,  1.9995],\n",
      "        [ 3.0000, -1.0000]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 4.9997, -1.0000], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.9984,  5.0000], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.9997], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class NetB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2,2)\n",
    "        self.fc2 = nn.Linear(2,1)\n",
    " \n",
    "    def forward(self, t):\n",
    "        t = self.fc1(t)\n",
    "        t = torch.relu(t)\n",
    "        t = self.fc2(t)\n",
    "        t = torch.sigmoid(t)\n",
    "        return t\n",
    "\n",
    "def loss_fn(y, y_hat):\n",
    "    return 0.5*(y-y_hat)**2\n",
    "\n",
    "w11, w12, b1, w21, w22, b2, w31, w32, b3 = [-3, 2, 5, 3, -1, -1, -1, 5, -1]\n",
    "\n",
    "net = NetB()\n",
    "net.fc1.weight.data = torch.tensor([[w11, w12], [w21, w22]]).float()\n",
    "net.fc1.bias.data = torch.tensor([b1, b2]).float()\n",
    "\n",
    "net.fc2.weight.data = torch.tensor([w31, w32]).float()\n",
    "net.fc2.bias.data = torch.tensor([b3]).float()\n",
    "\n",
    "y = torch.tensor([1]).reshape(1,1)\n",
    "x = torch.tensor([1.0,2]).reshape(1,1,2)\n",
    "\n",
    "lr = 0.3\n",
    "\n",
    "print(\"\\nAusgangslage:\")\n",
    "for p in net.parameters():\n",
    "    print(p)\n",
    "    \n",
    "loss = loss_fn(net(x), y)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nGradienten:\")\n",
    "for p in net.parameters():\n",
    "    print(p.grad)\n",
    "\n",
    "optimizer.step()\n",
    "print(\"\\nUpdated Parameter, learningrate =\", lr)\n",
    "for p in net.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NetzC\n",
    "\n",
    "<img src=\"./img/netzCa.png\" width=\"601\">\n",
    "\n",
    "<img src=\"./img/netzC.png\" width=\"701\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2,2)\n",
    "        self.fc2 = nn.Linear(2,2)\n",
    "        self.fc3 = nn.Linear(2,1)\n",
    " \n",
    "    def forward(self, t):\n",
    "        t = self.fc1(t)\n",
    "        t = torch.relu(t)\n",
    "        t = self.fc2(t)\n",
    "        t = torch.relu(t)\n",
    "        t = self.fc3(t)\n",
    "        t = torch.sigmoid(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training für die OR-Funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2,1)\n",
    " \n",
    "    def forward(self, t):\n",
    "        t = self.fc1(t)\n",
    "        t = torch.sigmoid(t)\n",
    "        return t\n",
    "\n",
    "def loss_fn(y, y_hat):\n",
    "    return 0.5*(y-y_hat)**2\n",
    "\n",
    "\n",
    "X  = [[0,0],[0,1],[1,0],[1,1.]]\n",
    "Y = [0,1,1,1]\n",
    "\n",
    "X = torch.Tensor(X).reshape(4,1,1,2)   # nr, kanäle, width, height\n",
    "Y = torch.Tensor(Y).reshape(4,1,1)\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
