{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronale Netze\n",
    "\n",
    "Problemstellung Klassifikation: [Cat vs. Dog](https://www.kaggle.com/c/dogs-vs-cats/overview). Es stehen 25000 Beispielbilder zur Verfügung. \n",
    "\n",
    "Problemstellung Klassifikation: Ein Punkt soll in eine von zwei Klassen eingeteilt werden. Es stehen Beispielpunkte zur Verfügung.\n",
    "\n",
    "[ConvNetJS](https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html) - \n",
    "[Tensorflow Playground](https://playground.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "Ein Perceptron erhält Inputwerte x1, x2 und berechnet daraus eine gewichtete Summe. Wenn diese Summe\n",
    "größer als ein Schwellwert ist, bedeutet es, dass das Perceptron aktiviert wird und \"feuert\", d.h. eine 1 ausgibt. \n",
    "\n",
    "<img src=\"./img/nn_01.png\" width=\"800\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Gleichung bringen wir den Schwellwert auf die andere Seite. Den negativen Schwellwert nennen wir den **Bias b**. Je größer der Bias, desto einfacher ist es, das Perceptron zu aktivieren. Der Ausgabewert des Perceptrons hängt von den Inputwerten x1, x2 und den drei Parametern w1, w2 und b ab.\n",
    "\n",
    "<img src=\"./img/nna_01.png\" width=\"800\"/>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self,param):\n",
    "        self.w1, self.w2, self.b = param\n",
    "    \n",
    "    def calc(self, x1, x2):\n",
    "        z = self.w1 * x1 + self.w2 * x2 + self.b\n",
    "        if z >= 0: return 1\n",
    "        else: return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Perceptron([1,-2,-4])\n",
    "print(pc.calc(3,2))\n",
    "print(pc.calc(3,-2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir möchten ein Perceptron, das sich bei 0-1 Inputs wie ein logisches **OR** verhält. \n",
    "\n",
    "<img src=\"./img/nn_02.png\" width=\"300\"/>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [(0,0),(0,1),(1,0),(1,1)]\n",
    "Y = [0,1,1,1]\n",
    "pc = Perceptron([1,1,-0.5])\n",
    "for (x1,x2),y in zip(X,Y):\n",
    "    print(x1,x2,pc.calc(x1,x2),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Übung\n",
    "\n",
    "Bestimme die Parameter eines Perceptrons, das für die Punkte (2/4) und (6/3) feuert, aber nicht für (6/4). Überprüfe das Ergebnis in Python. Nutze dazu die Perceptron-Klasse von oben.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir möchten, dass das Perceptron geeignete Werte für seine Parameter selbst findet (lernt). Wir starten mit zufälligen Anfangswerten und messen den Fehler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [(0,0),(0,1),(1,0),(1,1)]\n",
    "Y = [0,1,1,1]\n",
    "pc = Perceptron([-1,1,-1.5])\n",
    "for (x1,x2),y in zip(X,Y):\n",
    "    print(x1,x2,pc.calc(x1,x2),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Perceptron macht viele Fehler. Wenn wir ein bisschen an den Parametern drehen, können wir leider nicht sehen, ob wir uns in die richtige Richtung bewegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [(0,0),(0,1),(1,0),(1,1)]\n",
    "Y = [0,1,1,1]\n",
    "pc1 = Perceptron([-1.1, 1, -1.5])\n",
    "pc2 = Perceptron([-0.9, 1, -1.5])\n",
    "for (x1,x2),y in zip(X,Y):\n",
    "    print(x1,x2,pc1.calc(x1,x2),pc2.calc(x1,x2), y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Neuron nutzt eine Aktivierungfunktion, die es erlaubt, bei Parameteränderungen zu erkennen, ob der Fehler größer oder kleiner wird.  Zunächst nutzen wir die **sigmoid**-Funktion als Aktivierungsfunktion.\n",
    "\n",
    "<img src=\"./img/nna_02.png\" width=\"700\"/>  \n",
    "\n",
    "<img src=\"./img/nn_03.png\" width=\"500\"/>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Neuron:\n",
    "    def __init__(self,param):\n",
    "        self.w1, self.w2, self.b = param\n",
    "    \n",
    "    def calc(self, x1, x2):\n",
    "        z = self.w1 * x1 + self.w2 * x2 + self.b\n",
    "        return 1/(1+math.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.18242552380635635 0.18242552380635635 0\n",
      "0 1 0.3775406687981454 0.3775406687981454 1\n",
      "1 0 0.06913842034334682 0.08317269649392238 1\n",
      "1 1 0.16798161486607552 0.19781611144141825 1\n"
     ]
    }
   ],
   "source": [
    "X = [(0,0),(0,1),(1,0),(1,1)]\n",
    "Y = [0,1,1,1]\n",
    "n1 = Neuron([-1.1, 1, -1.5])\n",
    "n2 = Neuron([-0.9, 1, -1.5])\n",
    "for (x1,x2),y in zip(X,Y):\n",
    "    print(x1,x2,n1.calc(x1,x2),n2.calc(x1,x2), y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir messen den Fehler, den das Neuron bei einer Berechnung macht, als den halben quadratischen Abstand zum\n",
    "gewünschten Ergebnis.\n",
    "\n",
    "<img src=\"./img/nna_03.png\" width=\"900\"/>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = 1, 0\n",
    "y = 1\n",
    "\n",
    "n1 = Neuron([-1.1, 1, -1.5])\n",
    "n2 = Neuron([-0.9, 1, -1.5])\n",
    "\n",
    "a = n1.calc(x1,x2)\n",
    "loss = 0.5*(y -a)**2\n",
    "print(loss)\n",
    "\n",
    "a = n2.calc(x1,x2)\n",
    "loss = 0.5*(y -a)**2\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Gesamtfehler wird über alle Eingaben gemittelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [(0,0),(0,1),(1,0),(1,1)]\n",
    "Y = [0,1,1,1]\n",
    "n1 = Neuron([-1.1, 1, -1.5])\n",
    "n2 = Neuron([-0.9, 1, -1.5])\n",
    "loss1 = loss2 = 0\n",
    "for (x1,x2),y in zip(X,Y):\n",
    "    a1 = n1.calc(x1,x2)\n",
    "    a2 = n2.calc(x1,x2)\n",
    "    loss1 = loss1 + 0.5*(y-a1)**2\n",
    "    loss2 = loss2 + 0.5*(y-a2)**2\n",
    "loss1 = loss1 / 4\n",
    "loss2 = loss2 / 4\n",
    "print(loss1, loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Übung\n",
    "\n",
    "Die erwartete Ausgabe für das Neuron mit w1,w2,b = -1,1,-1.5 bei der Eingabe von (3,2) ist 1. Wie groß ist der Fehler?\n",
    "Ändere durch Ausprobieren die Parameter w1,w2,b so, dass der Fehler kleiner als 0.000008 wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = 3, 2\n",
    "y = 1\n",
    "\n",
    "n = Neuron([-1, 1, -1.5])\n",
    "a = n.calc(x1,x2)\n",
    " \n",
    "loss = 0.5*(y -a)**2\n",
    "print(a,loss,loss<0.000008)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lokale Ableitungen\n",
    "\n",
    "Wir möchte wissen, wie sich bei Änderung des Parameters der *loss* ändert. Diese Frage wird durch die Ableitung beantwortet. Wenn wir mehrere Funktionen hintereinanderschalten, benötigen wir die Kettenregel. Bei einfachen Verkettungen lässt sich die Ableitungsfunktion berechnen, bei komplizierteren Verkettungen können wir die Ableitung an den Stellen, die uns interessieren, schrittweise berechnen, indem wir die lokalen Ableitungen der Teilfunktionen berechnen.\n",
    "\n",
    "<img src=\"./img/nn-kettenregel.png\" width=\"900\"/>   \n",
    "\n",
    "Wenn man w and der Stelle 1 ein klein wenig schubst, ändert sich a um das 36-fache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 27.036012\n"
     ]
    }
   ],
   "source": [
    "def f(w):\n",
    "    z = 2*w + 1\n",
    "    a = 3*z**2\n",
    "    return a\n",
    "\n",
    "w = 1\n",
    "a = f(1)\n",
    "\n",
    "w1 = 1.001  # Änderung von a um 0.001\n",
    "a1 = f(w1)\n",
    "print(a, a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Der Computationgraph des Neurons mit den lokalen Ableitungen\n",
    "\n",
    "Wir berechnen die lokalen Ableitungen für die Teilfunktionen in dem Computationgraph des Neurons.\n",
    "\n",
    "Übung: Zeige $\\sigma^{\\prime}(z) = \\sigma(z) \\cdot (1-\\sigma(z))$\n",
    "\n",
    "\n",
    "<img src=\"./img/nn-lokaleAbleitungen.png\" width=\"900\"/>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06478577619527111"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2 = 1, 0\n",
    "y = 1\n",
    "n = Neuron([-1, 1, -1.5])\n",
    "a = n.calc(x1,x2)\n",
    "\n",
    "dw1 = x1 * a * (1-a) * (a-y)\n",
    "dw1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ableitung gibt uns einen Hinweis darauf, in welche Richtung wir uns bewegen sollen, um den Fehler zu minimieren. Sie gibt keine Hinweis darauf, wie groß dieser Schritt sein soll. Das legen wir mit der *learning rate* fest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "Um herauszufinden, in welchem Verhältnis wir die Parameter ändern müssen, damit der Fehler geringer wird, nutzen wir den **Gradienten**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Fehler ist abhängig von den 3 Parametern w1, w2 und b. Wir versuchen den Fehler zu minimieren. Der Gradient gibt die Richtung des steilsten Anstiegs an. Um unseren Funktionswert zu minimieren, gehen wir mit unseren Parametern in Richtung des negativen Gradienten (**gradient descent**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Um den Gradienten einer Funktion zu berechnen, stellen wir sie in einem **computation graph** dar. Wir notieren die partiellen Ableitungen (lokaler Gradient) an den entsprechenden Verbindungen. Den Gradienten erhalten wir durch Multiplikation der lokalen Gradienten längs des entsprechenden Pfades.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Beispiel 1:\n",
    "\n",
    "Bestimme den Wert der Funktion Funktion $f(x_1,x_2,x_3) = (x_1^2 + 3x_2) \\cdot 2x_3$ am Punkt $x = (2,-1,3)$.\n",
    "Gehe von $x$ aus um $0.01$ Längeneinheiten in Richtung des negativen Gradienten zum Punkt $x'$ und werte dort die Funktion erneut aus.\n",
    "\n",
    "<img src=\"./img/nn_19.png\" width=\"800\"/>    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel 2:\n",
    "Bestimme den Wert der Funktion $f(x_1,x_2,x_3) = (x_1\\cdot x_2 + x_1^2) \\cdot (x_3 - 5)$ am Punkt $x = (1,-2,2)$.\n",
    "Gehe von $x$ aus um $0.01$ Längeneinheiten in Richtung des negativen Gradienten zum Punkt $x'$ und werte dort die Funktion erneut aus.\n",
    "(Mit etwas Übung kann man bei einfachen Funktionen die partiellen Ableitungen direkt hinschreiben). \n",
    "\n",
    "<img src=\"./img/nn_20.png\" width=\"800\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Beispiel 3: \n",
    "Wenn es auf dem Weg zurück zur Variablen mehrere Wege gibt, werden die Werte der Pfade addiert.\n",
    "\n",
    "Bestimme den Wert der Funktion $f(x_1,x_2,x_3) = (x_1 + 2x_2)(x_2 +4x_3)$ am Punkt $x = (-1,2,3)$. Gehe von $x$ aus um $0.02$ Längeneinheiten in Richtung des negativen Gradienten zum Punkt $x'$ und werte dort die Funktion erneut aus.\n",
    "\n",
    "<img src=\"./img/nn_21.png\" width=\"900\"/>   \n",
    "\n",
    "In diesem Beispiel ist es noch möglich, die partiellen Ableitungen direkt hinzuschreiben. Die Addition der Pfade spiegelt sich in der Addition bei der Produktregel.\n",
    "\n",
    "<img src=\"./img/nn_21a.png\" width=\"500\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Übung\n",
    "\n",
    "Bestimme den Wert der Funktion $f(x_1,x_2,x_3) = (x_1^2 -4x_3)(4x_1x_2 - x_3)$  am Punkt $x = (1,2,-1)$. Gehe von $x$ aus um $0.1$ Längeneinheiten in Richtung des negativen Gradienten zum Punkt $x'$ und werte dort die Funktion erneut aus.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die lokalen Gradienten eines Neurons\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./img/nn_08.png\" width=\"701\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir fassen alles zu einer Einheit zusammen\n",
    "\n",
    "<img src=\"./img/nn_22.png\" width=\"601\"/>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Neuron:\n",
    "    def __init__(self,param):\n",
    "        self.w1, self.w2, self.b = param\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        z = self.w1 * x1 + self.w2 * x2 + self.b\n",
    "        a = 1/(1+math.exp(-1.0*z))\n",
    "        \n",
    "        # Wir merken uns die lokalen Gradienten\n",
    "        self.db = a *(1-a)\n",
    "        self.dw1 = x1 * self.db   \n",
    "        self.dw2 = x2 * self.db\n",
    "        self.dx1 = self.w1 * self.db\n",
    "        self.dx2 = self.w2 * self.db\n",
    "        return a\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1cdf35a63e16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdw2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "n.w1, n.w2, n.b\n",
    "a = n.forward(3,2)\n",
    "a, n.dw1, n.dw2, n.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Übung\n",
    "\n",
    "Berechne für $x = (3,2)$ den Output und die lokalen Gradienten des Neurons <br>\n",
    "```n = Neuron([-1,2,-4])``` <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fehlerberechnung Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Berechnung des Fehlers als computational graph\n",
    "\n",
    "<img src=\"./img/nn_15.png\" width=\"601\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel\n",
    "\n",
    "Für das Neuron  &nbsp; ```n = Neuron([-1,2,-4])```  &nbsp; ist $y = 1$ der erwartete Output bei $x = (-1,2)$.\n",
    "Berechne die neuen Werte für $w_1, w_2, b$ nach einem Durchgang forward/backward-propagation bei einer learning-rate von 0.2\n",
    "\n",
    "<img src=\"./img/nn_23.png\" width=\"801\"/>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self,param):\n",
    "        self.w1, self.w2, self.b = param\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        z = self.w1 * x1 + self.w2 * x2 + self.b\n",
    "        print(\"z=\",z)\n",
    "        a = 1/(1+math.exp(-1.0*z))\n",
    "        \n",
    "        # wir berechnen die lokalen Gradienten\n",
    "        self.db = a *(1-a)    \n",
    "        print(\"a*(1-a)\",self.db)\n",
    "        self.dw1 = x1 * self.db   \n",
    "        self.dw2 = x2 * self.db\n",
    "        #self.dx1 = self.w1 * self.db\n",
    "        #self.dx2 = self.w2 * self.db\n",
    "        return a\n",
    "    \n",
    "    def backward(self, g):\n",
    "        # wir multiplizieren den lokalen Gradienten mit\n",
    "        # dem upstream Gradienten g\n",
    "        self.db *= g\n",
    "        self.dw1 *= g\n",
    "        self.dw2 *= g\n",
    "        #self.dx1 *= g\n",
    "        #self.dx2 *= g\n",
    "        \n",
    "    def update(self,lr):\n",
    "        self.w1 = self.w1 - lr * self.dw1\n",
    "        self.w2 = self.w2 - lr * self.dw2\n",
    "        self.b = self.b - lr * self.db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Neuron([-1,1,-1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z= -2.5\n",
      "a*(1-a) 0.07010371654510815\n",
      "-- nach forward propagation ---\n",
      "a = 0.0759\n",
      "db = 0.0701\n",
      "dw1 = 0.0701, dw2 = 0.0000\n",
      "loss=0.4270\n",
      "g=-0.9241\n",
      "-- nach backward propagation ---\n",
      "db = -0.0648\n",
      "dw1 = -0.0648, dw2 = -0.0000\n",
      "-- nach Parameterupdate ---\n",
      "b = -1.4981\n",
      "w1 = -0.9981, w2 = 1.0000\n",
      "-- neues Ergebnis:\n",
      "z= -2.4961128534282837\n",
      "a*(1-a) 0.07033518368744232\n",
      "a = 0.0761\n"
     ]
    }
   ],
   "source": [
    "n = Neuron([-1,1,-1.5])\n",
    "a = n.forward(1,0)\n",
    "print('-- nach forward propagation ---') \n",
    "print(\"a = {:6.4f}\".format(a))\n",
    "print(\"db = {:6.4f}\".format(n.db))\n",
    "print(\"dw1 = {:6.4f}, dw2 = {:6.4f}\".format(n.dw1,n.dw2))\n",
    "\n",
    "y = 1\n",
    "print(\"loss={:6.4f}\".format(0.5*(y-a)**2))\n",
    "g = a - y  # upstream gradient\n",
    "print(\"g={:6.4f}\".format(g))\n",
    "n.backward(g)\n",
    "print('-- nach backward propagation ---')\n",
    "print(\"db = {:6.4f}\".format(n.db))\n",
    "print(\"dw1 = {:6.4f}, dw2 = {:6.4f}\".format(n.dw1,n.dw2))\n",
    "\n",
    "print('-- nach Parameterupdate ---')\n",
    "n.update(0.03)\n",
    "print(\"b = {:6.4f}\".format(n.b))\n",
    "print(\"w1 = {:6.4f}, w2 = {:6.4f}\".format(n.w1,n.w2))\n",
    "\n",
    "print('-- neues Ergebnis:') \n",
    "a = n.forward(1,0)\n",
    "print(\"a = {:6.4f}\".format(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ein Neuron für die OR-Funktion\n",
    "\n",
    "Wir initialisieren die Parameter des Neurons mit zufälligen Werten. Wir spendieren dem Neuron noch weitere Attribute, damit es\n",
    "die Änderungswünsche der vier Eingabepunkte aufsummieren kann. Die Änderung der Parameter orientiert sich dann an dem Durchschnitt aller Änderungswünsche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self,param):\n",
    "        self.w1, self.w2, self.b = param\n",
    "        self.sumdb = 0\n",
    "        self.sumdw1 = 0\n",
    "        self.sumdw2 = 0\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        z = self.w1 * x1 + self.w2 * x2 + self.b\n",
    "        a = 1/(1+math.exp(-1.0*z))\n",
    "        \n",
    "        # wir berechnen die lokalen Gradienten\n",
    "        self.db = a *(1-a)    \n",
    "        self.dw1 = x1 * self.db   \n",
    "        self.dw2 = x2 * self.db\n",
    "        self.dx1 = self.w1 * self.db\n",
    "        self.dx2 = self.w2 * self.db\n",
    "        return a\n",
    "    \n",
    "    def backward(self, g):\n",
    "        \n",
    "        # wir multiplizieren den lokalen Gradienten mit\n",
    "        # dem upstream Gradienten g\n",
    "        self.db *= g\n",
    "        self.dw1 *= g\n",
    "        self.dw2 *= g\n",
    "        self.dx1 *= g\n",
    "        self.dx2 *= g\n",
    "        \n",
    "        # wir addieren die Änderungswünsche\n",
    "        self.sumdb += self.db \n",
    "        self.sumdw1 += self.dw1\n",
    "        self.sumdw2 += self.dw2 \n",
    "        \n",
    "    def update(self,lr):\n",
    "        self.w1 = self.w1 - lr * self.sumdw1\n",
    "        self.w2 = self.w2 - lr * self.sumdw2\n",
    "        self.b = self.b - lr * self.sumdb\n",
    "        \n",
    "        # reset Änderungswünsche für neuen Durchgang\n",
    "        self.sumdw1 = 0\n",
    "        self.sumdw2 = 0\n",
    "        self.sumdb = 0\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18703072998505568\n",
      "0.391121969113219\n",
      "0.0793838691612706\n",
      "0.1940468317366638\n"
     ]
    }
   ],
   "source": [
    "n = Neuron([-1, 1, -1.5])\n",
    "lr = 0.1\n",
    "\n",
    "X  = [(0,0),(0,1),(1,0),(1,1)]\n",
    "Y = [0,1,1,1]\n",
    "for k in range(1):\n",
    "    for (x1,x2),y in zip(X,Y):\n",
    "        a = n.forward(x1,x2)\n",
    "        n.backward(a-y)\n",
    "    n.update(lr)\n",
    "\n",
    "for (x1,x2) in X:\n",
    "    print(n.forward(x1,x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def plotGerade(w1, w2, b):\n",
    "    m = -w1/w2\n",
    "    c = -b/w2\n",
    "    y0 = c\n",
    "    y1 = m + c\n",
    "    plt.plot([0,1],[y0,y1])\n",
    "    plt.scatter([0,0,1,1], [0,1,0,1], s=30)\n",
    "    \n",
    "plotGerade(n.w1, n.w2, n.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berechnen der Parameter für eine XOR-Erkennung\n",
    "\n",
    "Mit einem Neuron können wir kein **XOR** modellieren. Wie vernetzen 3 Neuronen und lassen das Netz die 9 Parameter lernen.\n",
    "\n",
    "\n",
    "<img src=\"./img/nn_16.png\" width=\"400\"/>  \n",
    "\n",
    "Da nur die Layer mit Parametern gezählt werden, ist das Netz ein 2-Layer Netz. Für die hidden Layer eignet sich die \n",
    "**Relu** Aktivierungsfunktion besser.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu \n",
    "\n",
    "Relu = rectified linear unit, $f(x) = max(0,x)$\n",
    "\n",
    "<img src=\"./img/nn_17.png\" width=\"400\"/>  \n",
    "\n",
    "Der lokale Gradient der Relu-Funktion\n",
    "\n",
    "<img src=\"./img/nn_18.png\" width=\"400\"/>  \n",
    "\n",
    "<img src=\"./img/nn_24.png\" width=\"900\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Netz implementiert die abgebildeten Verknüpfungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Netz:\n",
    "    def __init__(self,param):\n",
    "        self.w11, self.w12, self.b1, self.w21, self.w22, self.b2, self.w31, self.w32, self.b3 = param\n",
    "   \n",
    "    def forward(self, x1, x2):\n",
    "        z1 = self.w11 * x1 + self.w12 * x2 + self.b1   \n",
    "        a1 = z1 if z1 > 0 else 0 \n",
    "        \n",
    "        z2 = self.w21 * x1 + self.w22 * x2 + self.b2 \n",
    "        a2 = z2 if z2 > 0 else 0 \n",
    "        \n",
    "        z3 = self.w31 * a1 + self.w32 * a2 + self.b3\n",
    "        a3 = 1/(1+math.exp(-1.0*z3))\n",
    "        \n",
    "        # wir berechnen die lokalen gradienten \n",
    "        self.db3 = a3 *(1-a3)    \n",
    "        self.dw31 = db3*a1 * self.db3   \n",
    "        self.dw32 = a2 * self.db3\n",
    "        self.d1 = self.w1 * self.db\n",
    "        self.dx2 = self.w2 * self.db\n",
    "        \n",
    "        \n",
    "        \n",
    "        g = a3 - y\n",
    "        \n",
    "        # backward\n",
    "        db3 =  g * a3*(1-a3)\n",
    "        dw31 = db3 * a1\n",
    "        dw32 = db3 * a2\n",
    " \n",
    "        db1 = db3 * self.w31 if z1 > 0 else 0\n",
    "        dw11 = db1 * x1\n",
    "        dw12 = db1 * x2\n",
    "        \n",
    "        db2 = db3 * self.w32 if z2 > 0 else 0\n",
    "        dw21 = db2 * x1\n",
    "        dw22 = db2 * x2\n",
    "        \n",
    "        return a3\n",
    "    \n",
    "    def update(self, lr):\n",
    "        \n",
    "        self.w11 = self.w11 - lr * self.dw11\n",
    "        self.w12 = self.w12 - lr * self.dw12\n",
    "        self.b1 = self.b1 - lr * self.db1\n",
    "        self.w21 = self.w21 - lr * self.dw21\n",
    "        self.w22 = self.w22 - lr * self.dw22\n",
    "        self.b2 = self.b2 - lr * self.db2\n",
    "        self.w31 = self.w31 - lr * self.dw31\n",
    "        self.w32 = self.w32 - lr * self.dw32\n",
    "        self.b3 = self.b3 - lr * self.db3\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Netz:\n",
    "    def __init__(self,param):\n",
    "        self.w11, self.w12, self.b1, self.w21, self.w22, self.b2, self.w31, self.w32, self.b3 = param\n",
    "   \n",
    "    def calc(self, x1, x2):\n",
    "        # forward\n",
    "        z1 = self.w11 * x1 + self.w12 * x2 + self.b1   \n",
    "        a1 = z1 if z1 > 0 else 0 \n",
    "        \n",
    "        z2 = self.w21 * x1 + self.w22 * x2 + self.b2 \n",
    "        a2 = z2 if z2 > 0 else 0 \n",
    "        \n",
    "        z3 = self.w31 * a1 + self.w32 * a2 + self.b3\n",
    "        a3 = 1/(1+math.exp(-1.0*z3))\n",
    "        \n",
    "        g = a3 - y\n",
    "        \n",
    "        # backward\n",
    "        db3 =  g * a3*(1-a3)\n",
    "        dw31 = db3 * a1\n",
    "        dw32 = db3 * a2\n",
    " \n",
    "        db1 = db3 * self.w31 if z1 > 0 else 0\n",
    "        dw11 = db1 * x1\n",
    "        dw12 = db1 * x2\n",
    "        \n",
    "        db2 = db3 * self.w32 if z2 > 0 else 0\n",
    "        dw21 = db2 * x1\n",
    "        dw22 = db2 * x2\n",
    "        \n",
    "        return a3\n",
    "    \n",
    "    def update(self, lr):\n",
    "        \n",
    "        self.w11 = self.w11 - lr * self.dw11\n",
    "        self.w12 = self.w12 - lr * self.dw12\n",
    "        self.b1 = self.b1 - lr * self.db1\n",
    "        self.w21 = self.w21 - lr * self.dw21\n",
    "        self.w22 = self.w22 - lr * self.dw22\n",
    "        self.b2 = self.b2 - lr * self.db2\n",
    "        self.w31 = self.w31 - lr * self.dw31\n",
    "        self.w32 = self.w32 - lr * self.dw32\n",
    "        self.b3 = self.b3 - lr * self.db3\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [-3, 2, 5, 3, -1, -1, -1, 5, -1]\n",
    "netz = Netz(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir erweitern die Klasse Netz um die Summenfelder für die Änderungswünsche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Netz:\n",
    "    def __init__(self,param):\n",
    "        self.w11, self.w12, self.b1, self.w21, self.w22, self.b2, self.w31, self.w32, self.b3 = param\n",
    "        self.sumdw11 = self.sumdw12 = self.sumdb1 = self.sumdw21 = self.sumdw22 = self.sumdb2 = 0\n",
    "        self.sumdw31 = self.sumdw32 = self.sumdb3 = 0\n",
    "   \n",
    "    def calc(self, x1, x2, y=0):\n",
    "        \n",
    "        # forward\n",
    "        z1 = self.w11 * x1 + self.w12 * x2 + self.b1 \n",
    "        a1 = z1 if z1 > 0 else 0 \n",
    "        \n",
    "        z2 = self.w21 * x1 + self.w22 * x2 + self.b2 \n",
    "        a2 = z2 if z2 > 0 else 0 \n",
    "        \n",
    "        z3 = self.w31 * a1 + self.w32 * a2 + self.b3\n",
    "        a3 = 1/(1+math.exp(-1.0*z3))\n",
    "        #print(a1)\n",
    "        \n",
    "        g = a3 - y\n",
    "     \n",
    "        # backward\n",
    "        db3 =  g * a3*(1-a3)\n",
    "        dw31 = db3 * a1\n",
    "        dw32 = db3 * a2\n",
    " \n",
    "        db1 = db3 * self.w31 if z1 > 0 else 0\n",
    "        dw11 = db1 * x1\n",
    "        dw12 = db1 * x2\n",
    "        \n",
    "        db2 = db3 * self.w32 if z2 > 0 else 0\n",
    "        dw21 = db2 * x1\n",
    "        dw22 = db2 * x2\n",
    "        \n",
    "        self.sumdb1 += db1\n",
    "        self.sumdw11 += dw11\n",
    "        self.sumdw12 += dw12\n",
    "        \n",
    "        self.sumdb2 += db2\n",
    "        self.sumdw21 += dw21\n",
    "        self.sumdw22 += dw22\n",
    "        \n",
    "        self.sumdb3 += db3\n",
    "        self.sumdw31 += dw31\n",
    "        self.sumdw32 += dw32\n",
    "        \n",
    "        return a3\n",
    "    \n",
    "    def update(self, lr):\n",
    "        \n",
    "        self.w11 = self.w11 - lr * self.sumdw11\n",
    "        self.w12 = self.w12 - lr * self.sumdw12\n",
    "        self.b1 = self.b1 - lr * self.sumdb1\n",
    "        self.w21 = self.w21 - lr * self.sumdw21\n",
    "        self.w22 = self.w22 - lr * self.sumdw22\n",
    "        self.b2 = self.b2 - lr * self.sumdb2\n",
    "        self.w31 = self.w31 - lr * self.sumdw31\n",
    "        self.w32 = self.w32 - lr * self.sumdw32\n",
    "        self.b3 = self.b3 - lr * self.sumdb3\n",
    "        \n",
    "        self.sumdw11 = self.sumdw12 = self.sumdb1 = self.sumdw21 = self.sumdw22 = self.sumdb2 = 0\n",
    "        self.sumdw31 = self.sumdw32 = self.sumdb3 = 0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0024726231566347743\n",
      "0.0003353501304664781\n",
      "0.9990889488055994\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "X  = [(0,0),(0,1),(1,0),(1,1)]\n",
    "Y = [0,1,1,0]\n",
    "param = [-3, 2, 5, 3, -1, -1, -1, 5, -1]        # bei 1 noch schlecht, bei 1000 schlecht, bei 10000 gut\n",
    "#param = [-3, 2, -5, 3, -1, -1, -1, 5, -1]      # kommt zu keinem richtigen Ergebnis.\n",
    "lr = 0.1\n",
    "netz = Netz(param)\n",
    "for i in range(1):\n",
    "    for (x1,x2),y in zip(X,Y):\n",
    "        netz.calc(x1,x2,y)\n",
    "    #netz.update(0.1)\n",
    "for (x1,x2),y in zip(X,Y):\n",
    "    print(netz.calc(x1,x2,y))\n",
    "#print(netz.w31,netz.w32,netz.b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x1 = 0\n",
    "X1, X2, Y = [], [], []\n",
    "while x1 < 1:\n",
    "    x2 = 0\n",
    "    while x2 < 1:\n",
    "        x2 += 0.05\n",
    "        X1.append(x1)\n",
    "        X2.append(x2)\n",
    "        Y.append(netz.calc(x1,x2) > 0.5)\n",
    "    x1 += 0.05\n",
    "\n",
    "plt.scatter(X1, X2, c=Y,s=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
