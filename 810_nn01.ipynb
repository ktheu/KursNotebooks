{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronale Netze\n",
    "\n",
    "Problemstellung Klassifikation: [Cat vs. Dog](https://www.kaggle.com/c/dogs-vs-cats/overview). Es stehen 25000 Beispielbilder zur Verfügung. \n",
    "\n",
    "Problemstellung Klassifikation: Ein Punkt soll in eine von zwei Klassen eingeteilt werden. Es stehen Beispielpunkte zur Verfügung.\n",
    "\n",
    "[ConvNetJS](https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html) - \n",
    "[Tensorflow Playground](https://playground.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "Ein Perceptron erhält Inputwerte x1, x2 und berechnet daraus eine gewichtete Summe. Wenn diese Summe\n",
    "größer als ein Schwellwert ist, bedeutet es, dass das Perceptron aktiviert wird und \"feuert\", d.h. eine 1 ausgibt. \n",
    "\n",
    "<img src=\"./img/nn_01.png\" width=\"800\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Gleichung bringen wir den Schwellwert auf die andere Seite. Den negativen Schwellwert nennen wir den **Bias b**. Je größer der Bias, desto einfacher ist es, das Perceptron zu aktivieren. Der Ausgabewert des Perceptrons hängt von den Inputwerten x1, x2 und den drei Parametern w1, w2 und b ab.\n",
    "\n",
    "<img src=\"./img/nna_01.png\" width=\"800\"/>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "X = [(3,2),(3,-2)]\n",
    "w1, w2, b = 1, -2, -4   # Parameter des Perceptrons\n",
    "\n",
    "for x1, x2 in X:\n",
    "    z = w1 * x1 + w2 * x2 + b\n",
    "    a = 1 if z > 0 else 0\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir möchten ein Perceptron, das sich bei 0-1 Inputs wie ein logisches **OR** verhält. \n",
    "\n",
    "<img src=\"./img/nn_02.png\" width=\"300\"/>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0   0 0\n",
      "0 1   1 1\n",
      "1 0   1 1\n",
      "1 1   1 1\n"
     ]
    }
   ],
   "source": [
    "X = [(0,0),(0,1),(1,0),(1,1)]\n",
    "Y = [0,1,1,1]\n",
    "w1, w2, b = 1, 1, -0.5   # Parameter des Perceptrons\n",
    "\n",
    "for (x1, x2), y in zip(X,Y):\n",
    "    z = w1 * x1 + w2 * x2 + b\n",
    "    a = 1 if z > 0 else 0\n",
    "    print(x1,x2,' ',a,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n",
      "0 1 1\n",
      "1 0 1\n",
      "1 1 1\n"
     ]
    }
   ],
   "source": [
    "X = [(0,0),(0,1),(1,0),(1,1)]\n",
    "Y = [0,1,1,1]\n",
    "for (x1,x2),y in zip(X,Y):\n",
    "    print(x1,x2,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Übung\n",
    "\n",
    "Bestimme die Parameter eines Perceptrons, das für die Punkte (2/4) und (6/3) feuert, aber nicht für (6/4). Überprüfe das Ergebnis in Python. Nutze dazu die Perceptron-Klasse von oben.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir suchen einen Algorithmus, der geeignete Werte für die Parameter des Perceptrons findet. Wir starten mit zufälligen Anfangswerten und messen den Fehler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0   0 0\n",
      "0 1   0 1\n",
      "1 0   0 1\n",
      "1 1   0 1\n"
     ]
    }
   ],
   "source": [
    "X = [(0,0),(0,1),(1,0),(1,1)]\n",
    "Y = [0,1,1,1]\n",
    "w1, w2, b = -1, 1, -1.5   # zufällige Anfangsparameter\n",
    "\n",
    "for (x1, x2), y in zip(X,Y):\n",
    "    z = w1 * x1 + w2 * x2 + b\n",
    "    a = 1 if z > 0 else 0\n",
    "    print(x1,x2,' ',a,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit diesen Parametern machen wir viele Fehler. Wenn wir ein bisschen an den Parametern drehen, können wir leider nicht sehen, ob wir uns in die richtige Richtung bewegen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0   0 0\n",
      "0 1   0 1\n",
      "1 0   0 1\n",
      "1 1   0 1\n"
     ]
    }
   ],
   "source": [
    "w1, w2, b = -1.1, 1, -1.5   # w1 kleiner gemacht\n",
    "\n",
    "for (x1, x2), y in zip(X,Y):\n",
    "    z = w1 * x1 + w2 * x2 + b\n",
    "    a = 1 if z > 0 else 0\n",
    "    print(x1,x2,' ',a,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0   0 0\n",
      "0 1   0 1\n",
      "1 0   0 1\n",
      "1 1   0 1\n"
     ]
    }
   ],
   "source": [
    "w1, w2, b = -0.9, 1, -1.5   # w1 größer gemacht\n",
    "\n",
    "for (x1, x2), y in zip(X,Y):\n",
    "    z = w1 * x1 + w2 * x2 + b\n",
    "    a = 1 if z > 0 else 0\n",
    "    print(x1,x2,' ',a,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Neuron nutzt eine Aktivierungfunktion, die es erlaubt, bei Parameteränderungen zu erkennen, ob der Fehler größer oder kleiner wird.  Zunächst nutzen wir die **sigmoid**-Funktion als Aktivierungsfunktion.\n",
    "\n",
    "<img src=\"./img/nna_02.png\" width=\"700\"/>  \n",
    "\n",
    "<img src=\"./img/nn_03.png\" width=\"500\"/>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0  0.1824  0\n",
      "0 1  0.3775  1\n",
      "1 0  0.0759  1\n",
      "1 1  0.1824  1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "w1, w2, b = -1, 1, -1.5   # zufällige Anfangswerte\n",
    "\n",
    "for (x1, x2), y in zip(X,Y):\n",
    "    z = w1 * x1 + w2 * x2 + b\n",
    "    a = 1/(1+math.exp(-z))\n",
    "\n",
    "    print(\"{} {} {: 1.4f}  {}\".format(x1,x2,a,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0  0.1824  0\n",
      "0 1  0.3775  1\n",
      "1 0  0.0691  1\n",
      "1 1  0.1680  1\n"
     ]
    }
   ],
   "source": [
    "w1, w2, b = -1.1, 1, -1.5   # w1 kleiner gemacht\n",
    "\n",
    "for (x1, x2), y in zip(X,Y):\n",
    "    z = w1 * x1 + w2 * x2 + b\n",
    "    a = 1/(1+math.exp(-z))\n",
    "\n",
    "    print(\"{} {} {: 1.4f}  {}\".format(x1,x2,a,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0  0.1824  0\n",
      "0 1  0.3775  1\n",
      "1 0  0.0832  1\n",
      "1 1  0.1978  1\n"
     ]
    }
   ],
   "source": [
    "w1, w2, b = -0.9, 1, -1.5   # w1 größer gemacht\n",
    "\n",
    "for (x1, x2), y in zip(X,Y):\n",
    "    z = w1 * x1 + w2 * x2 + b\n",
    "    a = 1/(1+math.exp(-z))\n",
    "\n",
    "    print(\"{} {} {: 1.4f}  {}\".format(x1,x2,a,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können erkennen, die Vergrößerung von w1 das Ergebnis ein klein wenig verbessert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berechnung des Fehlers (loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir messen den Fehler, den das Neuron macht, als den halben quadratischen Abstand zum\n",
    "gewünschten Ergebnis.\n",
    "\n",
    "<img src=\"./img/nna_03.png\" width=\"900\"/>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0  0.1824  0 loss:  0.0166 \n",
      "0 1  0.3775  1 loss:  0.1937 \n",
      "1 0  0.0759  1 loss:  0.4270 \n",
      "1 1  0.1824  1 loss:  0.3342 \n"
     ]
    }
   ],
   "source": [
    "w1, w2, b = -1, 1, -1.5   # w1 größer gemacht\n",
    "\n",
    "for (x1, x2), y in zip(X,Y):\n",
    "    z = w1 * x1 + w2 * x2 + b\n",
    "    a = 1/(1+math.exp(-z))\n",
    "    loss = 0.5 * (y - a)**2\n",
    "\n",
    "    print(\"{} {} {: 1.4f}  {} loss: {: 1.4f} \".format(x1,x2,a,y,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Gesamtfehler berechnen wir den Mittelwert der einzelnen Fehler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0  0.1824  0 \n",
      "0 1  0.3775  1 \n",
      "1 0  0.0759  1 \n",
      "1 1  0.1824  1 \n",
      "loss:  0.2429\n"
     ]
    }
   ],
   "source": [
    "w1, w2, b = -1, 1, -1.5     # zufällige Anfangswerte\n",
    "loss = 0\n",
    "for (x1, x2), y in zip(X,Y):\n",
    "    z = w1 * x1 + w2 * x2 + b\n",
    "    a = 1/(1+math.exp(-z))\n",
    "    loss += 0.5 * (y - a)**2\n",
    "    print(\"{} {} {: 1.4f}  {} \".format(x1,x2,a,y))\n",
    "    \n",
    "print(\"loss: {: 1.4f}\".format(loss/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0  0.1824  0 \n",
      "0 1  0.3775  1 \n",
      "1 0  0.0832  1 \n",
      "1 1  0.1978  1 \n",
      "loss:  0.2381\n"
     ]
    }
   ],
   "source": [
    "w1, w2, b = -0.9, 1, -1.5   # w1 größer gemacht\n",
    "loss = 0\n",
    "for (x1, x2), y in zip(X,Y):\n",
    "    z = w1 * x1 + w2 * x2 + b\n",
    "    a = 1/(1+math.exp(-z))\n",
    "    loss += 0.5 * (y - a)**2\n",
    "    print(\"{} {} {: 1.4f}  {} \".format(x1,x2,a,y))\n",
    "    \n",
    "print(\"loss: {: 1.4f}\".format(loss/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der gesuchte Algorithmus wird versuchen, die Anfangsparameter so zu verändern, dass der *loss* möglichst klein wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lokale Ableitungen\n",
    "\n",
    "Wir möchte wissen, wie sich bei Änderung des Parameters der *loss* ändert. Diese Frage wird durch die Ableitung beantwortet. Wenn wir mehrere Funktionen hintereinanderschalten, benötigen wir die Kettenregel. Bei einfachen Verkettungen lässt sich die Ableitungsfunktion berechnen, bei komplizierteren Verkettungen können wir die Ableitung an den Stellen, die uns interessieren, schrittweise berechnen, indem wir die lokalen Ableitungen der Teilfunktionen berechnen.\n",
    "\n",
    "<img src=\"./img/nn-kettenregel.png\" width=\"900\"/>   \n",
    "\n",
    "Wenn man w and der Stelle 1 ein klein wenig schubst, ändert sich a um das 36-fache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 27.036012\n"
     ]
    }
   ],
   "source": [
    "def f(w):\n",
    "    z = 2*w + 1\n",
    "    a = 3*z**2\n",
    "    return a\n",
    "\n",
    "w = 1\n",
    "a = f(1)\n",
    "\n",
    "w1 = 1.001  # Änderung von a um 0.001\n",
    "a1 = f(w1)\n",
    "print(a, a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Der Computationgraph des Neurons mit den lokalen Ableitungen\n",
    "\n",
    "Wir berechnen die lokalen Ableitungen für die Teilfunktionen in dem Computationgraph des Neurons.\n",
    "\n",
    "Übung: Zeige $\\sigma^{\\prime}(z) = \\sigma(z) \\cdot (1-\\sigma(z))$\n",
    "\n",
    "\n",
    "<img src=\"./img/nn-lokaleAbleitungen.png\" width=\"900\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Wert der lokalen Ableitung bei w1 für den Input (1/0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0648\n"
     ]
    }
   ],
   "source": [
    "w1, w2, b = -1, 1, -1.5    \n",
    "x1, x2, y = 1, 0, 1\n",
    "\n",
    "z = w1 * x1 + w2 * x2 + b\n",
    "a = 1/(1+math.exp(-z))\n",
    "\n",
    "dw1 = (a - y) * a * (1 - a) * x1\n",
    "print(\"{:1.4f}\".format(dw1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ableitung gibt uns einen Hinweis darauf, in welche Richtung wir uns bewegen sollen, um den Fehler zu minimieren. Sie gibt keine Hinweis darauf, wie groß dieser Schritt sein soll. Das legen wir mit der *learning rate* fest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "Der Fehler ist abhängig von den 3 Parametern w1, w2 und b.\n",
    "\n",
    "Um herauszufinden, in welchem Verhältnis wir die Parameter ändern müssen, damit der Fehler geringer wird, nutzen wir den **Gradienten**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Gradient gibt die Richtung des steilsten Anstiegs an. Um den *loss* zu minimieren, gehen wir mit unseren Parametern in Richtung des negativen Gradienten (**gradient descent**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/gradient.png\" width=\"920\"/>   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beispiel: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: -0.0648 -0.0000 -0.0648\n"
     ]
    }
   ],
   "source": [
    "w1, w2, b = -1, 1, -1.5    \n",
    "x1, x2, y = 1, 0, 1\n",
    "\n",
    "z = w1 * x1 + w2 * x2 + b\n",
    "a = 1/(1+math.exp(-z))\n",
    "\n",
    "dw1 = (a - y) * a * (1 - a) * x1\n",
    "dw2 = (a - y) * a * (1 - a) * x2\n",
    "db = (a - y) * a * (1 - a)\n",
    "print(\"Gradient: {:1.4f} {:1.4f} {:1.4f}\".format(dw1, dw2, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Summe der Änderungswünsche aller Eingaben ergeben die Richtung für die Änderung der Parameter. Die Größe des Schritts legen wir\n",
    "mit der learningrate fest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Änderungen der Parameter: -0.1867 -0.2682 -0.3058\n"
     ]
    }
   ],
   "source": [
    "w1, w2, b = -1, 1, -1.5     # zufällige Anfangswerte\n",
    "dw1 = dw2 = db = 0\n",
    "for (x1, x2), y in zip(X,Y):\n",
    "    z = w1 * x1 + w2 * x2 + b\n",
    "    a = 1/(1+math.exp(-z))\n",
    "    \n",
    "    dw1 += (a - y) * a * (1 - a) * x1\n",
    "    dw2 += (a - y) * a * (1 - a) * x2\n",
    "    db  += (a - y) * a * (1 - a)\n",
    "    \n",
    "print(\"Änderungen der Parameter: {: 1.4f} {: 1.4f} {: 1.4f}\".format(dw1, dw2, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward / Backward \n",
    "\n",
    "Wir können uns die Berechnung als eine Vorwärts-Rückwärts-Bewegung vorstellen. In der Vorwärtsbewegung berechnen wir z und a und die lokalen Gradienten. Den letzten Gradienten (a - y) nennen wir *upstream gradient*. In der Rückwärtsbewegung multiplizieren wir den *upstream gradienten* mit den lokalen Gradienten auf dem Weg zu den Parametern. Dann werden die Parameter upgedated und das Spiel beginnt von vorne. \n",
    "Einen Durchgang nennt man *Epoche*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1, w2, b:      3.3934  3.4657 -1.4102\n",
      "loss:           0.0083 \n",
      "\n",
      "0 0  0.1962  0 \n",
      "0 1  0.8865  1 \n",
      "1 0  0.8790  1 \n",
      "1 1  0.9957  1 \n"
     ]
    }
   ],
   "source": [
    "X = [(0,0),(0,1),(1,0),(1,1)]\n",
    "Y = [0,1,1,1]\n",
    "lr = 0.1\n",
    "w1, w2, b = -1, 1, -1.5\n",
    "for i in range(1000):\n",
    "    dw1, dw2, db, loss = 0, 0, 0, 0   # hier sammeln wir die Änderungswünsche und den loss\n",
    "    for (x1,x2),y in zip(X,Y):\n",
    "        z = x1*w1 + x2*w2 + b          \n",
    "        a = 1/(1+math.exp(-z))\n",
    "        loss+= 0.5*(y-a)**2\n",
    "        db  += a*(1-a) * (a-y)        \n",
    "        dw1 += a*(1-a) * (a-y) * x1\n",
    "        dw2 += a*(1-a) * (a-y) * x2\n",
    "    w1 -= lr*dw1                      # update der Parameter\n",
    "    w2 -= lr*dw2\n",
    "    b  -= lr*db\n",
    "    \n",
    "print(\"{:14s} {: 1.4f} {: 1.4f} {: 1.4f}\".format(\"w1, w2, b:\",w1, w2, b))\n",
    "print(\"{:14s} {: 1.4f} \".format(\"loss:\",loss/4))\n",
    "print()\n",
    "\n",
    "for (x1,x2),y in zip(X,Y):\n",
    "    z = x1*w1 + x2*w2 + b          \n",
    "    a = 1/(1+math.exp(-z))\n",
    "    print(\"{} {} {: 1.4f}  {} \".format(x1,x2,a,y))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung\n",
    "\n",
    "Gegeben ist das Neuron mit den Parametern $w1=-1, w2=2, b=-4$. Berechne für $x = (3,2)$, einer learning-rage $lr=0.1$ und einem gewünschten Output $y = 1$ die Parameter des Neurons nach 2 Durchgängen (= nach dem 2. Update der Parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1, x2, y, lr:  3.0000  2.0000  1.0000  0.1000\n",
      "-------------------------\n",
      "w1, w2, b:     -0.9871  2.0086 -3.9957\n",
      "z, a, loss:    -3.0000  0.0474  0.4537\n",
      "a*(1-a), a-y:   0.0452 -0.9526\n",
      "dw1, dw2, db:  -0.1291 -0.0861 -0.0430\n",
      "-------------------------\n",
      "w1, w2, b:     -0.9735  2.0177 -3.9912\n",
      "z, a, loss:    -2.9398  0.0502  0.4510\n",
      "a*(1-a), a-y:   0.0477 -0.9498\n",
      "dw1, dw2, db:  -0.1359 -0.0906 -0.0453\n",
      "-------------------------\n",
      "w1, w2, b:     -0.9735  2.0177 -3.9912\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "x1, x2, y, lr = 3, 2, 1, 0.1\n",
    "print(\"{:14s} {: 1.4f} {: 1.4f} {: 1.4f} {: 1.4f}\".format(\"x1, x2, y, lr:\",x1, x2, y, lr))\n",
    "print(\"-------------------------\")\n",
    "\n",
    "w1, w2, b = -1, 2, -4\n",
    "z = x1*w1 + x2*w2 + b\n",
    "a = 1/(1+math.exp(-z))\n",
    "db = a*(1-a) * (a-y)\n",
    "dw1 = db * x1\n",
    "dw2 = db * x2\n",
    "w1 -= lr*dw1\n",
    "w2 -= lr*dw2\n",
    "b  -= lr*db\n",
    "print(\"{:14s} {: 1.4f} {: 1.4f} {: 1.4f}\".format(\"w1, w2, b:\",w1, w2, b))\n",
    "print(\"{:14s} {: 1.4f} {: 1.4f} {: 1.4f}\".format(\"z, a, loss:\",z, a, 0.5*(y-a)**2))\n",
    "print(\"{:14s} {: 1.4f} {: 1.4f}\".format(\"a*(1-a), a-y:\",a*(1-a), a-y))\n",
    "print(\"{:14s} {: 1.4f} {: 1.4f} {: 1.4f}\".format(\"dw1, dw2, db:\",dw1, dw2, db))\n",
    "print(\"-------------------------\")\n",
    "\n",
    "z = x1*w1 + x2*w2 + b\n",
    "a = 1/(1+math.exp(-z))\n",
    "db = a*(1-a) * (a-y)\n",
    "dw1 = db * x1\n",
    "dw2 = db * x2\n",
    "w1 -= lr*dw1\n",
    "w2 -= lr*dw2\n",
    "b  -= lr*db\n",
    "print(\"{:14s} {: 1.4f} {: 1.4f} {: 1.4f}\".format(\"w1, w2, b:\",w1, w2, b))\n",
    "print(\"{:14s} {: 1.4f} {: 1.4f} {: 1.4f}\".format(\"z, a, loss:\",z, a, 0.5*(y-a)**2))\n",
    "print(\"{:14s} {: 1.4f} {: 1.4f}\".format(\"a*(1-a), a-y:\",a*(1-a), a-y))\n",
    "print(\"{:14s} {: 1.4f} {: 1.4f} {: 1.4f}\".format(\"dw1, dw2, db:\",dw1, dw2, db))\n",
    "print(\"-------------------------\")\n",
    "print(\"{:14s} {: 1.4f} {: 1.4f} {: 1.4f}\".format(\"w1, w2, b:\",w1, w2, b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die XOR-Funktion\n",
    "\n",
    "Wir suchen nach demselben Verfahren ein Neuron für die XOR-Funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1, w2, b:      0.0000  0.0000 -0.0000\n",
      "loss:           0.1250 \n",
      "0 0  0.5000  0 \n",
      "0 1  0.5000  1 \n",
      "1 0  0.5000  1 \n",
      "1 1  0.5000  0 \n"
     ]
    }
   ],
   "source": [
    "X = [(0,0),(0,1),(1,0),(1,1)]\n",
    "Y = [0,1,1,0]\n",
    "lr = 0.1\n",
    "w1, w2, b = -1, 1, -1.5\n",
    "for i in range(10000):\n",
    "    dw1, dw2, db, loss = 0, 0, 0, 0   # hier sammeln wir die Änderungswünsche und den loss\n",
    "    for (x1,x2),y in zip(X,Y):\n",
    "        z = x1*w1 + x2*w2 + b\n",
    "        a = 1/(1+math.exp(-z))\n",
    "        loss+= 0.5*(y-a)**2\n",
    "        db  += a*(1-a) * (a-y)\n",
    "        dw1 += a*(1-a) * (a-y) * x1\n",
    "        dw2 += a*(1-a) * (a-y) * x2\n",
    "    w1 -= lr*dw1\n",
    "    w2 -= lr*dw2\n",
    "    b  -= lr*db\n",
    "print(\"{:14s} {: 1.4f} {: 1.4f} {: 1.4f}\".format(\"w1, w2, b:\",w1, w2, b))\n",
    "print(\"{:14s} {: 1.4f} \".format(\"loss:\", loss/4))\n",
    "\n",
    "for (x1,x2),y in zip(X,Y):\n",
    "    z = x1*w1 + x2*w2 + b          \n",
    "    a = 1/(1+math.exp(-z))\n",
    "    print(\"{} {} {: 1.4f}  {} \".format(x1,x2,a,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ein Netz von Neuronen für eine XOR-Erkennung\n",
    "\n",
    "Mit einem Neuron können wir kein **XOR** modellieren. Wie vernetzen 3 Neuronen und lassen das Netz die 9 Parameter lernen.\n",
    "\n",
    "\n",
    "<img src=\"./img/nn_16.png\" width=\"400\"/>  \n",
    "\n",
    "Da nur die Layer mit Parametern gezählt werden, ist das Netz ein 2-Layer Netz. Für die hidden Layer eignet sich die \n",
    "**Relu** Aktivierungsfunktion besser.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu \n",
    "\n",
    "Relu = rectified linear unit, $f(x) = max(0,x)$\n",
    "\n",
    "<img src=\"./img/nn_17.png\" width=\"400\"/>  \n",
    "\n",
    "Der lokale Gradient der Relu-Funktion\n",
    "\n",
    "<img src=\"./img/nn_18.png\" width=\"401\"/>  \n",
    "\n",
    "Ein Netz mit 3 Neuronen\n",
    "\n",
    "<img src=\"./img/nn-netz3.png\" width=\"921\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier nutzen wir für den berechneten Output die übliche Bezeichnung $\\hat{y}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:           0.0001\n",
      "0 0  0.0233  0\n",
      "0 1  0.9854  1\n",
      "1 0  0.9924  1\n",
      "1 1  0.0084  0\n",
      "\n",
      "Die Parameter für die Neuronen: \n",
      "n1: -4.7030  4.1614  1.0960\n",
      "n2:  3.0091 -2.0182 -0.9909\n",
      "n3:  1.9102  5.2999 -5.8286\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "X = [(0,0),(0,1),(1,0),(1,1)]\n",
    "Y = [0,1,1,0]\n",
    "\n",
    "lr = 0.1\n",
    "w11, w12, b1 = -3, 2, 5\n",
    "w21, w22, b2 =  3, -1, -1\n",
    "w31, w32, b3 = -1, 5, -1\n",
    "\n",
    " \n",
    "for i in range(10000):     # bei 1000 noch schlecht\n",
    "    dw11 = dw12 = db1 = dw21 = dw22 = db2 = dw31 = dw32 = db3 = loss = 0  \n",
    "    \n",
    "    for (x1,x2),y in zip(X,Y):\n",
    "\n",
    "        z1 = w11 * x1 + w12 * x2 + b1   \n",
    "        a1 = max(z1,0)\n",
    "  \n",
    "        z2 = w21 * x1 + w22 * x2 + b2   \n",
    "        a2 = max(z2,0)\n",
    "        \n",
    "        z3 = w31 * a1 + w32 * a2 + b3   \n",
    "        a3 = 1/(1+math.exp(-1.0*z3))\n",
    "    \n",
    "        loss += 0.5*(y-a3)**2\n",
    "        \n",
    "        db3  += (a3-y) * a3 * (1-a3)\n",
    "        dw31 += (a3-y) * a3 * (1-a3) * a1\n",
    "        dw32 += (a3-y) * a3 * (1-a3) * a2\n",
    "        \n",
    "        db1  += (a3-y) * a3 * (1-a3) * w31 if z1 > 0 else 0\n",
    "        dw11 += (a3-y) * a3 * (1-a3) * w31 * x1 if z1 > 0 else 0\n",
    "        dw12 += (a3-y) * a3 * (1-a3) * w31 * x2 if z1 > 0 else 0\n",
    "        \n",
    "        db2  += (a3-y) * a3 * (1-a3) * w32 if z2 > 0 else 0\n",
    "        dw21 += (a3-y) * a3 * (1-a3) * w32 * x1 if z2 > 0 else 0\n",
    "        dw22 += (a3-y) * a3 * (1-a3) * w32 * x2 if z2 > 0 else 0\n",
    "    \n",
    "    w11 -= lr * dw11\n",
    "    w12 -= lr * dw12\n",
    "    b1  -= lr * db1\n",
    "    w21 -= lr * dw21\n",
    "    w22 -= lr * dw22\n",
    "    b2  -= lr * db2\n",
    "    w31 -= lr * dw31\n",
    "    w32 -= lr * dw32\n",
    "    b3  -= lr * db3\n",
    "\n",
    "print(\"{:14s} {: 1.4f}\".format(\"loss:\", loss/4))        \n",
    " \n",
    "for (x1,x2),y in zip(X,Y):\n",
    "    \n",
    "    z1 = w11 * x1 + w12 * x2 + b1   \n",
    "    a1 = max(z1,0)\n",
    "\n",
    "    z2 = w21 * x1 + w22 * x2 + b2   \n",
    "    a2 = max(z2,0)\n",
    "    \n",
    "    z3= w31 * a1 + w32 * a2 + b3   \n",
    "    a3 = 1/(1+math.exp(-z3))\n",
    "\n",
    "    print(\"{} {}  {:1.4f}  {}\".format(x1,x2,a3,y))\n",
    "\n",
    "print(\"\\nDie Parameter für die Neuronen: \")\n",
    "print(\"n1: {: 1.4f} {: 1.4f} {: 1.4f}\".format(w11,w12,b1))\n",
    "print(\"n2: {: 1.4f} {: 1.4f} {: 1.4f}\".format(w21,w22,b2))\n",
    "print(\"n3: {: 1.4f} {: 1.4f} {: 1.4f}\".format(w31,w32,b3))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vektorisierung der Berechnung\n",
    "\n",
    "Für kleine Netze kann man die Berechnung wie oben mit for-Schleifen machen. Für größere Netze versucht man die inneren for-Schleifen durch Vektorisierung zu eliminieren. Die Berechnungen der einzelnen Durchgänge sind voneinander unabhängig und können parallel durchgeführt werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beispiel: Vektorisierung der Berechnung der gewichteten Summe\n",
    "\n",
    "<img src=\"./img/nn-vektorisierung.png\" width=\"721\"/>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5, -0.5, -2.5, -1.5]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,0,1,1], [0,1,0,1]]) \n",
    "W = np.array([[-1],[1]]) \n",
    "b = -1.5\n",
    "np.dot(W.T,X)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die GPUs (graphics processing units) sind auf solche Rechenoperationen spezialisiert, da ähnliche Berechnungen in der Computergraphik durchgeführt werden. Hier die Berechnung einer Rotation um 30 Grad.\n",
    "\n",
    "<img src=\"./img/nn-rotation1.png\" width=\"420\"/> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.23205081],\n",
       "       [1.8660254 ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = np.array([[np.sqrt(3)/2, -0.5], [0.5, np.sqrt(3)/2]])\n",
    "v = np.array([[2],[1]])\n",
    "np.dot(M,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/rotationsmatrix.png\" width=\"521\"/>   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
