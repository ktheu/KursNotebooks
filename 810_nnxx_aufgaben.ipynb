{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgaben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel 1:\n",
    "\n",
    "Bestimme den Wert der Funktion Funktion $f(x_1,x_2,x_3) = (x_1^2 + 3x_2) \\cdot 2x_3$ am Punkt $x = (2,-1,3)$.\n",
    "Gehe von $x$ aus um $0.01$ Längeneinheiten in Richtung des negativen Gradienten zum Punkt $x'$ und werte dort die Funktion erneut aus.\n",
    "\n",
    "<img src=\"./img/nn_19.png\" width=\"800\"/>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 6.0\n",
      "Gradient:  [24. 18.  2.]\n",
      "Länge des Gradienten: 30.066593\n",
      "Neuer Punkt:  [ 1.9920177 -1.0059867  2.9993348]\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0,-1.0,3.0],requires_grad=True)\n",
    "y = (x[0]*x[0] + 3*x[1])*2*x[2]\n",
    "print(\"y =\", y.detach().numpy())\n",
    "y.backward()\n",
    "grad = x.grad.numpy()\n",
    "print(\"Gradient: \",grad)\n",
    "gradLength = np.linalg.norm(grad)   \n",
    "print(\"Länge des Gradienten:\", gradLength )\n",
    "print(\"Neuer Punkt: \", x.detach().numpy() - 0.01 * grad/gradLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel 2:\n",
    "Bestimme den Wert der Funktion $f(x_1,x_2,x_3) = (x_1\\cdot x_2 + x_1^2) \\cdot (x_3 - 5)$ am Punkt $x = (1,-2,2)$.\n",
    "Gehe von $x$ aus um $0.01$ Längeneinheiten in Richtung des negativen Gradienten zum Punkt $x'$ und werte dort die Funktion erneut aus.\n",
    "(Mit etwas Übung kann man bei einfachen Funktionen die partiellen Ableitungen direkt hinschreiben). \n",
    "\n",
    "<img src=\"./img/nn_20.png\" width=\"800\"/>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 3.0\n",
      "Gradient:  [ 0. -3. -1.]\n",
      "Länge des Gradienten: 3.1622777\n",
      "Neuer Punkt:  [ 1.        -1.9905132  2.0031624]\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0,-2.0,2.0],requires_grad=True)\n",
    "y = (x[0]*x[1] + x[0]**2)*(x[2]-5)\n",
    "print(\"y =\", y.detach().numpy())\n",
    "y.backward()\n",
    "grad = x.grad.numpy()\n",
    "print(\"Gradient: \",grad)\n",
    "gradLength = np.linalg.norm(grad)   \n",
    "print(\"Länge des Gradienten:\", gradLength )\n",
    "print(\"Neuer Punkt: \", x.detach().numpy() - 0.01 * grad/gradLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Beispiel 3: \n",
    "Wenn es auf dem Weg zurück zur Variablen mehrere Wege gibt, werden die Werte der Pfade addiert.\n",
    "\n",
    "Bestimme den Wert der Funktion $f(x_1,x_2,x_3) = (x_1 + 2x_2)(x_2 +4x_3)$ am Punkt $x = (-1,2,3)$. Gehe von $x$ aus um $0.02$ Längeneinheiten in Richtung des negativen Gradienten zum Punkt $x'$ und werte dort die Funktion erneut aus.\n",
    "\n",
    "<img src=\"./img/nn_21.png\" width=\"900\"/>   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 42.0\n",
      "Gradient:  [14. 31. 12.]\n",
      "Länge des Gradienten: 36.069378\n",
      "Neuer Punkt:  [-1.0038815  1.9914055  2.996673 ]\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-1,2.0,3.0],requires_grad=True)\n",
    "y = (x[0]+2*x[1])*(x[1] + 4*x[2])\n",
    "print(\"y =\", y.detach().numpy())\n",
    "y.backward()\n",
    "grad = x.grad.numpy()\n",
    "print(\"Gradient: \",grad)\n",
    "gradLength = np.linalg.norm(grad)   \n",
    "print(\"Länge des Gradienten:\", gradLength )\n",
    "print(\"Neuer Punkt: \", x.detach().numpy() - 0.01 * grad/gradLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Übung\n",
    "\n",
    "Berechne für $x = (3,2)$ den Output und die lokalen Gradienten des Neurons <br>\n",
    "```n = Neuron([-1,2,-4])``` <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor(0.0474, grad_fn=<SigmoidBackward>)\n",
      "Gradient:  [0.13552998 0.09035332 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "x = torch.tensor([3.0,2.0],requires_grad=False)\n",
    "n = torch.tensor([-1.0,2.0,-4.0],requires_grad=True)\n",
    "z = x[0]*n[0] + x[1]*n[1] + n[2]\n",
    "a = torch.sigmoid(z)\n",
    "print(\"a =\", a)\n",
    "a.backward()\n",
    "grad = n.grad.numpy()\n",
    "print(\"Gradient: \",grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel\n",
    "\n",
    "Für das Neuron  &nbsp; ```n = Neuron([-1,2,-4])```  &nbsp; ist $y = 1$ der erwartete Output bei $x = (-1,2)$.\n",
    "Berechne die neuen Werte für $w_1, w_2, b$ nach einem Durchgang forward/backward-propagation bei einer learning-rate von 0.2\n",
    "\n",
    "<img src=\"./img/nn_23.png\" width=\"801\"/>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter:  [-1.  2. -4.]\n",
      "tensor([ 0.0529, -0.1058, -0.0529])\n",
      "Parameter:  [-1.0376815  2.0030801 -3.9984598]\n",
      "tensor([ 0.0490, -0.0979, -0.0490])\n",
      "Parameter:  [-1.0474743  2.0226657 -3.988667 ]\n",
      "tensor([ 0.0455, -0.0910, -0.0455])\n",
      "Parameter:  [-1.0565753  2.0408678 -3.9795659]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "x = torch.tensor([-1.,2.0],requires_grad=False)\n",
    "n = torch.tensor([-1., 2., -4.],requires_grad=True)\n",
    "print(\"Parameter: \", n.detach().numpy())\n",
    "\n",
    "for i in range(3):\n",
    "    z = x[0]*n[0] + x[1]*n[1] + n[2]\n",
    "    a = torch.sigmoid(z) \n",
    "    loss = 0.5*(1 - a)**2\n",
    "\n",
    "    loss.backward()\n",
    "    print(n.grad)\n",
    "    with torch.no_grad():\n",
    "        n -= 0.2 * n.grad\n",
    "        \n",
    "    print(\"Parameter: \", n.detach().numpy() - 0.2 * grad)\n",
    "    n.grad.zero_()\n",
    "    #b = -3.9796, w1 = -1.0204, w2 = 2.0407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgangslage:\n",
      "x1 = -1.0000, x2 = 2.0000\n",
      "b = -4.0000, w1 = -1.0000, w2 = 2.0000\n",
      "\n",
      "forward: -----\n",
      "a = 0.7311, y = 1.0000, a-y = -0.2689\n",
      "db = 0.1966, dw1 = -0.1966, dw2 = 0.3932\n",
      "backward: ----\n",
      "db = -0.0529, dw1 = 0.0529, dw2 = -0.1058\n",
      "b = -3.9894, w1 = -1.0106, w2 = 2.0212\n",
      "\n",
      "forward: -----\n",
      "a = 0.7433, y = 1.0000, a-y = -0.2567\n",
      "db = 0.1908, dw1 = -0.1908, dw2 = 0.3816\n",
      "backward: ----\n",
      "db = -0.0490, dw1 = 0.0490, dw2 = -0.0979\n",
      "b = -3.9796, w1 = -1.0204, w2 = 2.0407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self,param):\n",
    "        self.w1, self.w2, self.b = param\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        z = self.w1 * x1 + self.w2 * x2 + self.b\n",
    "        a = 1/(1+math.exp(-1.0*z))\n",
    "        \n",
    "        # wir berechnen die lokalen Gradienten\n",
    "        self.db = a *(1-a)    \n",
    "        self.dw1 = x1 * self.db   \n",
    "        self.dw2 = x2 * self.db\n",
    "        self.dx1 = self.w1 * self.db\n",
    "        self.dx2 = self.w2 * self.db\n",
    "        return a\n",
    "    \n",
    "    def backward(self, g):\n",
    "        \n",
    "        # wir multiplizieren den lokalen Gradienten mit\n",
    "        # dem upstream Gradienten g\n",
    "        self.db *= g\n",
    "        self.dw1 *= g\n",
    "        self.dw2 *= g\n",
    "        self.dx1 *= g\n",
    "        self.dx2 *= g\n",
    "        \n",
    "    def update(self,lr):\n",
    "        self.w1 = self.w1 - lr * self.dw1\n",
    "        self.w2 = self.w2 - lr * self.dw2\n",
    "        self.b = self.b - lr * self.db\n",
    "\n",
    "\n",
    "n = Neuron([-1,2,-4])\n",
    "y = 1                  # gewünschte Ausgabe\n",
    "x1, x2 = -1, 2\n",
    "lr = 0.2               # learning rate\n",
    "\n",
    "print('Ausgangslage:')\n",
    "print('x1 = {:5.4f}, x2 = {:5.4f}'.format(x1, x2)) \n",
    "print('b = {:5.4f}, w1 = {:5.4f}, w2 = {:5.4f}'.format(n.b, n.w1, n.w2)) \n",
    "print()\n",
    "\n",
    "for i in range(2):\n",
    "    print('forward: -----')\n",
    "    a = n.forward(x1,x2)\n",
    "    print('a = {:5.4f}, y = {:5.4f}, a-y = {:5.4f}'.format(a,y,a-y))\n",
    "    print('db = {:5.4f}, dw1 = {:5.4f}, dw2 = {:5.4f}'.format(n.db, n.dw1, n.dw2))  \n",
    "\n",
    "    print('backward: ----')\n",
    "    n.backward(a-y)\n",
    "    n.update(lr)\n",
    "\n",
    "    print('db = {:5.4f}, dw1 = {:5.4f}, dw2 = {:5.4f}'.format(n.db, n.dw1, n.dw2)) \n",
    "    print('b = {:5.4f}, w1 = {:5.4f}, w2 = {:5.4f}'.format(n.b, n.w1, n.w2)) \n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
