{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architekturen\n",
    "\n",
    "#### LeNet-5 (LeCun 1998)\n",
    "\n",
    "<img src=\"./img/cnn03.png\" width=\"800\">\n",
    "\n",
    " \n",
    " \n",
    "Damals war *padding* noch nicht verbreitet. Moderne Varianten nutzen am Ende eine Softmax-Funktion und max-Pooling statt avg-Pooling.\n",
    "\n",
    "Was heute noch gilt: Zuerst eine Folge von CNN-Layers, dann eine Folge von FC-Layers. Bei den CNN-Layers vermindert sich die Höhe und Breite der Bilder mit der Tiefe und die Anzahl der Kanäle steigt.\n",
    "\n",
    "LeNet-5 hatte ca. 60000 Parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlexNet (Alex Krizhesvky 2012)\n",
    "\n",
    "<img src=\"./img/cnn04.png\" width=\"901\"> \n",
    "\n",
    "AlexNet hatte ca 60 Millionen Parameter. LeNet nutzte noch Sigmoid oder Tanh-Aktivierungsfunktionen, AlexNet nutzte Relu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG-16 (2015)\n",
    "\n",
    "VGG-16 besteht aus 16 Layers (Pooling wird nicht mitgezählt). Es hat ca 138 Millionen Parameter. Der regelhafte Aufbau ist Vorbild für viele anderen Netzwerke geworden.\n",
    "\n",
    "<img src=\"./img/cnn05.png\" width=\"901\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet (2015)\n",
    "\n",
    "Netze mit sehr vielen Layern sind schwer zu trainieren. Es hat sich herausgestellt, dass dies besser gelingt, wenn man *skip connections* einführt. Das sind zusätzliche Verbindungen, die Layer überspringen. Daraus sind die ResNets entstanden. *ResNet34* und *ResNet50* sind sehr leistungsfähige Architekturen. Für diese Netze gibt es fertige Parameter, die 1000 Dinge unterscheiden können.\n",
    "\n",
    "<img src=\"./img/nn-resnet.png\" width=\"900\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Will man einen eigenen Klassifier entwickeln, kann man Transfer-Learning nutzen. Dazu nimmt man ein fertig trainiertes Netz (z.B. ResNet34), friert die Gewichte der Anfangslayer ein (d.h. die werden nicht mehr verändert) und trainiert nur die Gewichte der letzten Layer. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
