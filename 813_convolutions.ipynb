{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um Netzwerk zu verbessern, möchte man gerne weitere Layers anfügen (das Netzwerk soll tiefer werden). Um zu verhindern, dass die Anzahl der zu trainierenden Parameter zu stark ansteigt, fasst man über die Konvolution-Operation benachbarte Werten zu einem Wert zusammen. \n",
    "\n",
    "<img src=\"./img/convolution01.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -5,  -4],\n",
       "          [-10,  -2]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [3,0,1,2],\n",
    "    [1,5,8,9],\n",
    "    [2,7,2,5],\n",
    "    [0,1,3,1]\n",
    "])\n",
    "\n",
    "filter = torch.tensor([[1, 0, -1],\n",
    "                       [1, 0, -1],\n",
    "                       [1, 0, -1]])\n",
    "\n",
    "x = x.reshape(1,1,4,4)\n",
    "filter = filter.reshape(1, 1, 3, 3)\n",
    "F.conv2d(x, filter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge detection\n",
    "\n",
    "Wir stellen uns die Bilderkennung so vor, dass die Filter in frühen Layers eines CNN einfache Features erkenne, spätere Layer erkennen kompliziertere Strukturen.\n",
    "\n",
    "<img src=\"./img/convolution02.png\" width=\"700\">\n",
    "\n",
    "Das Beispiel zeigt, wie sich mit einem Filter vertikale Linien erkennen lassen.\n",
    "\n",
    "<img src=\"./img/convolution03.png\" width=\"700\">\n",
    "\n",
    "Das Resultat der Convolution-Operation hebt die vertikalen Linie hervor. \n",
    "\n",
    "<img src=\"./img/convolution04.png\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0, 30, 30,  0],\n",
       "          [ 0, 30, 30,  0],\n",
       "          [ 0, 30, 30,  0],\n",
       "          [ 0, 30, 30,  0]]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [10,10,10,0,0,0],\n",
    "    [10,10,10,0,0,0], \n",
    "    [10,10,10,0,0,0], \n",
    "    [10,10,10,0,0,0],\n",
    "    [10,10,10,0,0,0],\n",
    "    [10,10,10,0,0,0]\n",
    "]) \n",
    "\n",
    "filter = torch.tensor([[1, 0, -1],\n",
    "                       [1, 0, -1],\n",
    "                       [1, 0, -1]])\n",
    "\n",
    "x = x.reshape(1,1,6,6)\n",
    "filter = filter.reshape(1, 1, 3, 3)\n",
    "F.conv2d(x, filter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weitere Beispiele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0,  0,  0,  0],\n",
       "          [30, 30, 30, 30],\n",
       "          [30, 30, 30, 30],\n",
       "          [ 0,  0,  0,  0]]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [10,10,10,10,10,10],\n",
    "    [10,10,10,10,10,10], \n",
    "    [10,10,10,10,10,10], \n",
    "    [0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0]\n",
    "]) \n",
    "\n",
    "filter = torch.tensor([[1, 1, 1],\n",
    "                       [0, 0, 0],\n",
    "                       [-1, -1, -1]])\n",
    "\n",
    "x = x.reshape(1,1,6,6)\n",
    "filter = filter.reshape(1, 1, 3, 3)\n",
    "F.conv2d(x, filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  0,   0,   0,   0],\n",
       "          [ 30,  10, -10, -30],\n",
       "          [ 30,  10, -10, -30],\n",
       "          [  0,   0,   0,   0]]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [10,10,10,0,0,0],\n",
    "    [10,10,10,0,0,0], \n",
    "    [10,10,10,0,0,0], \n",
    "    [0,0,0,10,10,10],\n",
    "    [0,0,0,10,10,10],\n",
    "    [0,0,0,10,10,10]\n",
    "]) \n",
    "\n",
    "filter = torch.tensor([[1, 1, 1],\n",
    "                       [0, 0, 0],\n",
    "                       [-1, -1, -1]])\n",
    "\n",
    "x = x.reshape(1,1,6,6)\n",
    "filter = filter.reshape(1, 1, 3, 3)\n",
    "F.conv2d(x, filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man kann auch Varianten probieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_filter = torch.tensor([[1, 0, -1],\n",
    "                             [2, 0, -2],\n",
    "                             [1, 0, -1]])\n",
    "\n",
    "sobel_filter = torch.tensor([[3, 0, -3],\n",
    "                             [10, 0, -10],\n",
    "                             [3, 0, -3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Häufig lässt man das Netzwerk selbst geeignete Werte der Filter finden. Die Werte der Filter sind dann zu trainierende Parameter. Filter werden auch manchmal *kernel* genannt. *kernel_size* ist der Parameter, mit dem wir in Pytorch die Größe eines zu trainierenden Filters definieren.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding\n",
    "\n",
    "In den bisherigen Beispielen war die Ausgabe der Convolusion eine kleinere Matrix als die Ausgangsmatrix. \n",
    "\n",
    "```\n",
    "Input: n\n",
    "Filter: f\n",
    "Output: n-f+1\n",
    "```\n",
    "\n",
    "Nachteile:\n",
    "- das Bild wird immer kleiner\n",
    "- die Randpixel gehen weniger häufig in die Ausgabe ein.\n",
    "\n",
    "Um die Nachteile zu verhindern, wird das Ursprungsbild mit einem Rand versehen, üblicherweise mit 0-Werten (Padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -5,  -5,  -6,   9],\n",
       "          [-12,  -5,  -4,  11],\n",
       "          [-13, -10,  -2,  13],\n",
       "          [ -8,  -3,   2,   5]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hier ist das Padding manuell eingefügt\n",
    "x = torch.tensor([\n",
    "    [0,0,0,0,0,0],\n",
    "    [0,3,0,1,2,0],\n",
    "    [0,1,5,8,9,0],\n",
    "    [0,2,7,2,5,0],\n",
    "    [0,0,1,3,1,0],\n",
    "    [0,0,0,0,0,0]\n",
    "]) \n",
    "\n",
    "filter = torch.tensor([[1, 0, -1],\n",
    "                       [1, 0, -1],\n",
    "                       [1, 0, -1]])\n",
    "\n",
    "x = x.reshape(1,1,6,6)\n",
    "filter = filter.reshape(1, 1, 3, 3)\n",
    "F.conv2d(x, filter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Funktion *conv2d* können wir dazu einen *padding*-Parameter mitgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -5,  -5,  -6,   9],\n",
       "          [-12,  -5,  -4,  11],\n",
       "          [-13, -10,  -2,  13],\n",
       "          [ -8,  -3,   2,   5]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hier wird das Padding über einen Parameter gesteuert.\n",
    "x = torch.tensor([\n",
    "    [3,0,1,2],\n",
    "    [1,5,8,9],\n",
    "    [2,7,2,5],\n",
    "    [0,1,3,1]\n",
    "])\n",
    "\n",
    "filter = torch.tensor([[1, 0, -1],\n",
    "                       [1, 0, -1],\n",
    "                       [1, 0, -1]])\n",
    "\n",
    "x = x.reshape(1,1,4,4)\n",
    "filter = filter.reshape(1, 1, 3, 3)\n",
    "F.conv2d(x, filter,padding=(1,1))     # Anzahl Zeros auf beiden Seiten jeder Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Input: n\n",
    "Filter: f\n",
    "Padding: p\n",
    "Output: n+2p-f+1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Konvolusion ohne Padding heißt *valid*-Konvolusion. <br>\n",
    "Eine Konvolusion mit gleicher Größe von Input und Output heißt *same*-Konvolusion.  \n",
    "\n",
    "Für eine *same*-Konvolusion gilt:\n",
    "\n",
    "$$p = \\frac{f-1}{2}$$\n",
    "\n",
    "f ist so gut wie immer ungerade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stride\n",
    "\n",
    "Mit *striding* steuern wir, wieviel Schritte der Filter bis zum nächsten Halt macht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-5, -4],\n",
       "          [-8,  7]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [3,0,1,2,8],\n",
    "    [1,5,8,9,4],\n",
    "    [2,7,2,5,3],\n",
    "    [0,1,3,1,2],\n",
    "    [4,2,9,3,2],\n",
    "])\n",
    "\n",
    "filter = torch.tensor([[1, 0, -1],\n",
    "                       [1, 0, -1],\n",
    "                       [1, 0, -1]])\n",
    "\n",
    "x = x.reshape(1,1,5,5)\n",
    "filter = filter.reshape(1, 1, 3, 3)\n",
    "F.conv2d(x, filter,stride=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Input: n\n",
    "Filter: f\n",
    "Padding: p\n",
    "Stride: s\n",
    "Output: (n+2p-f)/s + 1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Volume-Convolutions\n",
    "\n",
    "Für ein Farbbild benötigen wir einen 3-dimensionalen Filter. Die Konvolusion-Operation addiert die Werte, die der Filter-Würfel liefert, zu einer Zahl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[20, 18],\n",
       "          [25, 45]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [[3,0,1],\n",
    "     [1,5,8],\n",
    "     [2,7,2]],\n",
    "     \n",
    "    [[0,1,2],\n",
    "     [5,2,5],\n",
    "     [0,1,3]],\n",
    "    \n",
    "    [[2,0,1],\n",
    "     [2,2,3],\n",
    "     [2,1,1]]\n",
    "])\n",
    "\n",
    "filter = torch.tensor([\n",
    "    [[1, 2],\n",
    "     [1, 0]],\n",
    "    \n",
    "    [[0, 1],\n",
    "     [1, 0]],\n",
    "    \n",
    "    [[3, 1],\n",
    "     [0, 2]]\n",
    "])\n",
    "      \n",
    "\n",
    "x = x.reshape(1,3,3,3)\n",
    "filter = filter.reshape(1, 3, 2, 2)\n",
    "F.conv2d(x, filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beispiel: Ein 6x6x3 Input mit einem 3x3x3 Filter ergibt einen 4x4x1 Output:\n",
    "\n",
    "<img src=\"./img/convolution05.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mehrere Filter\n",
    "\n",
    "Um mehrere Features parallel zu entdecken, wendet man mehrere Filter gleichzeitig an und stapelt die jeweiligen Outputs hintereinander.\n",
    "\n",
    "<img src=\"./img/convolution06.png\" width=\"600\">\n",
    "\n",
    "Beispiel: Wenn ein 32x32x3 Bild mit 10 3x3x3 Filtern über Konvolution verknüpft wird (ohne padding und stride), dann entsteht daraus ein\n",
    "30x30x10 Output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer\n",
    "\n",
    "Ein Layer eines CNN besteht darin, den Output der Konvolution mit einem Bias zu Versehen, durch eine Aktivierungsfunktion zu schicken und die Resultate hintereinander zu stapeln.\n",
    "\n",
    "<img src=\"./img/convolution07.png\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Übung: Wieviele Parameter hat ein CNN-Layer, der aus 10 3x3x3 Filtern besteht?  Ist die Anzahl der Parameter abhängig von der Größe des Inputs? (Antwort: 280, nein)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 25, 25])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Übung: was wird ausgegeben?\n",
    "\n",
    "inputs = torch.randn(5,3,50,50)\n",
    "filters = torch.randn(8,3,4,4)\n",
    "output = F.conv2d(inputs, filters, padding = (1,1), stride = 2)\n",
    "output.shape\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ein CNN-Layer für das Fashion-MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        \n",
    "     \n",
    "    def forward(self, t):\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 6, 24, 24])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "inputs = torch.randn(20,1,28,28)   # Das Format eines Fashion-MNIST Batches der Größe 20\n",
    "output = net(inputs)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# die Anzahl der zu trainierenden Parameter\n",
    "for x in list(net.conv1.parameters()):\n",
    "    print(x.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Übung\n",
    "```\n",
    "CNN-Layer 1: f=3,s=1,p=0, anzFilter = 10 \n",
    "CNN-Layer 2: f=5,s=2,p=0, anzFilter = 20\n",
    "CNN-Layer 3: f=5,s=2,p=0, anzFilter = 40\n",
    "\n",
    "Input: 20x3x39x39\n",
    "Welche Dimension hat der Output?\n",
    "Antwort: 20x40x7x7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 40, 7, 7])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=20, out_channels=40, kernel_size=5, stride=2)\n",
    "     \n",
    "    def forward(self, t):\n",
    "        t = self.conv1(t)\n",
    "        t = self.conv2(t)\n",
    "        t = self.conv3(t)\n",
    "     \n",
    "        return t\n",
    "\n",
    "net = Net()\n",
    "input = torch.randn(20,3,39,39)\n",
    "output = net(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Kunst beim Design von CNNs ist es, die richtigen Hyperparameter zu wählen: wieviele Filter, welche Größe sollen sie haben, Werte für geeignete Wert für padding und stride finden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling\n",
    "\n",
    "In CNNs gibt es neben den Convolution-Layers üblicherweise auch Pooling-Layers. Am häufigsten findet man *max-pooling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[3., 9.],\n",
      "          [2., 4.]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [3,0,1,2],\n",
    "    [1,2,8,9],\n",
    "    [2,1,2,4],\n",
    "    [0,1,3,1]\n",
    "]).float()\n",
    "\n",
    "x = x.reshape(1,1,4,4)\n",
    "output = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max-Pooling hat sich in vielen Fällen als vorteilhaft erweisen. Versuch einer Erklärung: Wenn eine hohe Zahl bedeutet, dass ein Feature entdeckt wurde, dann sorgt max-Pooling dafür, dass diese Entdeckung bestehen bleibt (und nicht durch umliegende kleinere Zahlen wieder in Frage gestellt wird (sort of)).\n",
    "\n",
    "Max-Pooling Layers führen keine zusätzliche zu trainierenden Parameter ein (sie werden ausschließlich durch ihre Hyperparameter festgelegt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vorteile von CNN-Layers\n",
    "\n",
    "CNN-Layers haben deutlich weniger Parameter als fully-connected-Layers\n",
    "\n",
    "<img src=\"./img/convolution08.png\" width=\"400\">\n",
    "\n",
    "Fully-connected-Layer: ca. 14 Millionen Parameter, \n",
    "CNN-Layer: 156 Parameter\n",
    "\n",
    "Wie kommt die Einsparung von Parametern zustande?\n",
    "\n",
    "**Parameter sharing**:\n",
    "Wir stellen uns einen Filter als einen feature-detector vor(z.B. vertikale Kanten). Wenn ein solcher feature-detector in einem Teil des Bildes nützlich ist, dann vermutlich auch in einem anderen Teil des Bildes. Aus der Menge der Input-Signale können also unterschiedliche Bereiche dieselben Paramter benutzen, um features zu entdecken. Deswegen ist es sinnvoll, sich mit demselben Filter über den gesamten Input zu bewegen.\n",
    "\n",
    "**Sparse connections**: Ein Output-Wert hängt nur noch von einem Teil der Input-Werte ab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
