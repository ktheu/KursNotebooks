{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax und Cross Entropy Loss \n",
    "long version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Sigmoid-Aktivierung nutzen wir, wenn wir zwei Klassen unterscheiden wollen (0,1). Die Softmax-Funktion nutzen wir als Aktivierungsfunktion für den letzten Layer bei mehr als 2 Klassen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wird ein neuronales Netz zur Klassifikation benutzt, dann nennt man den letzten Layer mit Neuronen häufig den\n",
    "**logits-layer**. Er produziert einen Vektor aus reellen Zahlen zwischen minus unendlich und plus unendlich, für jede Klassifikationklasse eine Zahl. Als Aktivierungsfunktion wird häufig die **softmax** -Funktion benutzt. Die Aufgabe der Softmax-Funktion ist es, den Vektor umzuwandeln in einen Vektor aus Wahrscheinlichkeiten dafür, dass die jeweilige Klasse erkannt wurde.\n",
    "\n",
    "\n",
    "Beispiel: Punkte in der Ebene werden mit drei Farben gefärbt. Ein neuronales Netz soll die Farbe eines Punktes vorhersagen.\n",
    "\n",
    "Der logits-Layer liefert für einen Input folgende Ausgabe:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([[2, -1, 0.1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je höher die Zahl, desto größer die Wahrscheinlichkeit, dass der Input zu der jeweiligen Klasse gehört.\n",
    "Die softmax-Funktion rechnet die Werte in Wahrscheinlichkeiten um, d.h. in Zahlen zwischen 0 und 1, die sich zusammen zur 1 addieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8338, 0.0415, 0.1247]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(logits,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie werden diese Zahlen berechnet? Da Wahrscheinlichkeiten immer positiv sind, werden zunächst alle drei Zahlen mit der e-Funktion in eine positive Zahl verwandelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.38905609893065\n",
      "0.36787944117144233\n",
      "1.1051709197224806\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "a = list(logits[0])\n",
    "for x in a:\n",
    "    print(math.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann werden die Zahlen noch normiert, damit sie sich zu 1 aufsummieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8337810127228958\n",
      "0.04151151228426165\n",
      "0.12470747499284246\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "a = list(logits[0]) \n",
    "summe = sum([math.exp(x) for x in a])\n",
    "for x in a:\n",
    "    print(math.exp(x)/summe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir mehrere Eingaben in einem Batch durch das Netz geschickt haben, enthält der logits-Layer Ausgaben für jedes Batch-Element. Die Softmax-Ausgabe erstellt für jeden Input einen Vektor mit den zugehörigen Wahrscheinlichkeiten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8338, 0.0415, 0.1247],\n",
       "        [0.4755, 0.2884, 0.2361]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([[2, -1, 0.1], [2.5,2,1.8]])\n",
    "torch.softmax(logits,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den loss bei einer Klassifikation zu messen wird häufig die cross-entropy-Funktion benutzt. Nehmen wir an, wir berechnen wie oben die Färbung eines Punktes, der drei Farben annehmen kann, die wir mit 0, 1, 2 kodieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0818)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor([2])\n",
    "logits = torch.tensor([[2,-1,0.1]])\n",
    "loss = F.cross_entropy(logits, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wird diese Zahl berechnet? Die cross_entropy Funktion wendet zunächst die softmax-Funktion auf die logits an\n",
    "(dies bedeutet, dass wir in der Definition unseres Netzes die Softmax-Funktion nicht als letzte Aktivierungfunktion\n",
    "definieren müssen, wenn wir den loss  mit der cross_entropy Funktion messen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8338, 0.0415, 0.1247]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = torch.softmax(logits,dim=1)\n",
    "sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann wird der Target-Wert in einen One-Hot-Vektor umgewandelt. Der One-Hot Vektor hat soviele Komponenten wie es Klassen gibt. Die Stelle mit der richtigen Klasse wird mit einer 1 belegt, alle anderen mit einer 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor([[0,0,1.0]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann wird das Skalarprodukt zwischen dem Softmax-Output und dem One-Hot-Vektor gebildet. Das Resultat ist die Wahrscheinlichkeit, die zur 1 gehört."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1247]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(sm,target.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann wird der negative Logarithmus dieser Zahl gebildet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0818]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- torch.log(torch.mm(sm,target.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Intuition\n",
    "\n",
    "In unserem Beispiel ist der loss $-log(0.1247)$. $0.1247$ ist die Wahrscheinlichkeit, mit der der Algorithmus die korrekte Aussage belegt. Das Ziel des Algorithmus ist es, denn loss zu minimieren. Das geht nur dadurch, dass die Zahl innerhalb des $-log(..)$ größer wird. Die Zahl kann (als Wahrscheinlichkeit) natürlich nicht größer als 1 werden, aber auf alle Fälle wird ein Algorithmus, der versucht, den loss zu minimieren, das dadurch versuchen, diese Zahl der 1 zu nähern. Das bedeutet eine Bewegung in Richtung korrekter Klassifizierung.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
